# Appendix e

A.2). 4.2 PLUG-IN APPROACH WITH NO CHANGES TO THE BASE LOSS As an alternative to modifying the base loss in Chow’s rule, we propose a new plug-in approach that seeks to directly mimic the optimal classiﬁer and rejector for the balanced L2R risk. Like standard Chow’s rule, we assume access to only a class probability estimator py(x) trained to minimize the softmax CE loss in Eq. (3). We then wish to approximate the Bayes-optimal classiﬁer and rejector from Theorem 1 by plugging in estimates py(x) of the class probabilities: ˆh(x) = arg max y∈[L] 1 ˆα[y] · py(x); (12) ˆr(x) = 1 ⇐⇒ max y∈[L] 1 ˆα[y] · py(x) < X y′∈[L]  1 ˆα[y′] −ˆµ[y′]  · py′(x) −c, (13) for choices of ˆα ∈(0, K)K and ˆµ ∈RK. One may carry out an exhaustive search to pick ˆα and ˆµ so that the resulting classiﬁer-rejector pair yields the lowest balanced error on a held-out sample. However, searching over 2K parameters can quickly become infeasible even when K is small. To make the search more efﬁcient, we exploit the additional constraint in Theorem 1 that requires ˆαk to match the rejector’s coverage for group Gk, i.e., ˆαk = K · P(y ∈Gk, ˆr(x) = 0). Speciﬁcally, we propose applying a grid search over only the multipliers ˆµ, while using a power-iteration-like heuristic to choose ˆα. Starting with initial values α(0) ∈(0, K)K, we propose alternating between constructing the classiﬁer-rejector pair and updating the group priors using the rejector’s coverage: • Construct (h(m), r(m)) using equations 12–13 with ˆα = α(m) and the ﬁxed multipliers ˆµ; • Set α(m+1) k = K · P(y ∈Gk, r(m)(x) = 0). One may repeat these steps until the iterates α(t)s converge to a stationary point, and return the corresponding classiﬁer and rejector. For moderate number of groups K, one may apply the above procedure for different values of multipliers ˆµ ∈RK, and pick the one for which the returned classiﬁer-rejector pair yields the lowest balanced error on a validation set. The details are outlined in Algorithm 1, where Λ is the set of multiplier values we wish to search over, and βk = 1 K for the balanced error. In fact, one can further prune the search space by re-parameterizing the rejection criterion in Eq. (13) to search over only K −1 parameters (se
