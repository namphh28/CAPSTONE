# Appendix F

for details). We train a Res Net-32 (50) model for CIFAR (Image Net and i Naturalist). 8
Published as a conference paper at ICLR 2024 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.1 0.2 0.3 0.4 0.5 0.6 Balanced Error Chow CSS Chow [BCE] Plug-in [Balanced] 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Balanced Error Chow CSS Chow [BCE] Plug-in [Balanced] 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 Balanced Error Chow CSS Chow [BCE] Plug-in [Balanced] 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.2 0.4 0.6 0.8 1.0 Worst-group Error Chow CSS Chow [DRO] Plug-in [Worst] (a) CIFAR-100 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.2 0.4 0.6 0.8 Worst-group Error Chow CSS Chow [DRO] Plug-in [Worst] (b) Image Net 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.1 0.2 0.3 0.4 Worst-group Error Chow CSS Chow [DRO] Plug-in [Worst] (c) i Naturalist Figure 3: Balanced and worst-group errors as functions of proportion of rejections. Lower is better. Evaluation metrics. For each dataset, we divide the classes into two groups, namely head and tail. We designate all classes with 20 or fewer samples in the training set as tail classes (Liu et al., 2019), and the remaining as the head. We evaluate both the balanced error and the worst-group error across the head and tail groups. We train classiÔ¨Åers and rejectors with different rejection costs c, and plot these metrics as a function of the proportion of rejections. We summarize the performance across different rejection rates by computing the areas under the respective curves (averaged over 5 trials); this summary metric is referred to as the area under risk-coverage curve (AURC) (Moon et al., 2020). Comparisons. We compare against two representative L2R baselines that optimize the standard 0-1 error: (i) Chow‚Äôs rule, which trains a base model by optimizing the CE loss, and thresholds the estimated max probability (Eq. (2)); and (ii) the cost-sensitive softmax (CSS) loss of Mozannar & Sontag (2020), which jointly trains a classiÔ¨Åer and rejector using a surrogate loss (with the Res Net model predicting L + 1 logits, and the (L + 1)-th denoting the ‚Äòabstain‚Äô option). We evaluate our plug-in methods for the balanced error (Alg. 1) and the worst-case error (Alg. 2), both of which use the same base model as Chow‚Äôs rule. We also evaluate variants of Chow‚Äôs rule (¬ß4.1), where the base model is trained with either a balanced cross-entropy (BCE) loss implemented through logit adjustments (Menon et al., 2021a), or the DRO loss prescribed by Jones et al. (2021). Results. We present plots of the balanced and worst-group errors as a function of the rejection rate (for varying values of c) in Figure 3. We summarize the area under these curves in Table 2. On all three datasets, the proposed plug-in methods are substantially better than both Chow‚Äôs rule and the CSS baseline. The plug-in methods are also comparable to or better than the variants of Chow‚Äôs rule that uses a balanced or DRO base loss. As shown in Figures 4‚Äì5 in Appendix F.5, Chow‚Äôs rule is unsurprisingly the best on the standard 0-1 error. On the other hand, Figures 6‚Äì7 show that our plug-in methods yield a lower gap between the tail and head errors compared to the L2R baselines. Finally, in Appendix F.6, we show on CIFAR-100 that the heuristic we employ in our plug-in approach to pick the group coverage Œ± (¬ß4.2) performs comparable to an exhaustive grid search over Œ±. 7 CONCLUSION We have shown that standard L2R can be sub-optimal for general evaluation metrics, and proposed simple plug-in methods that yield signiÔ¨Åcantly better trade-offs for both the balanced and worst-group error. Our methods require only a pre-trained model optimized with the CE loss, and are competitive with complex loss modiÔ¨Åcation approaches. In the future, it is of interest to explore joint training of the classiÔ¨Åer and rejector using surrogate losses (Cao et al., 2022), and extensions to hierarchical classiÔ¨Åcation (Wu et al., 2020b) and pairwise ranking (Shen et al., 2020; Mao et al., 2023b). 9
Published as a conference paper at ICLR 2024 REFERENCES Jean V Alves, Diogo Leit√£o, S√©rgio Jesus, Marco OP Sampaio, Javier Li√©bana, Pedro Saleiro, M√°rio AT Figueiredo, and Pedro Bizarro. Cost-sensitive learning to defer to multiple experts with workload constraints. ar Xiv preprint ar Xiv:2403.06906, 2024. Peter L Bartlett, Michael I Jordan, and Jon D Mc Auliffe. Convexity, classiÔ¨Åcation, and risk bounds. Journal of the American Statistical Association, 101(473):138‚Äì156, 2006. Kay Henning Brodersen, Cheng Soon Ong, Klaas Enno Stephan, and Joachim M. Buhmann. The balanced accuracy and its posterior distribution. In 2010 20th International Conference on Pattern Recognition, pp. 3121‚Äì3124, 2010. doi: 10.1109/ICPR.2010.764. Mateusz Buda, Atsuto Maki, and Maciej A. Mazurowski. A systematic study of the class imbalance problem in convolutional neural networks. ar Xiv:1710.05381 [cs, stat], October 2017. Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label-distribution-aware margin loss. In Advances in Neural Information Processing Systems, 2019. Yuzhou Cao, Tianchi Cai, Lei Feng, Lihong Gu, Jinjie Gu, Bo An, Gang Niu, and Masashi Sugiyama. Generalizing consistent multi-class classiÔ¨Åcation with rejection to be compatible with arbitrary losses. Advances in Neural Information Processing Systems, 35:521‚Äì534, 2022. Nontawat Charoenphakdee, Zhenghang Cui, Yivan Zhang, and Masashi Sugiyama. ClassiÔ¨Åcation with rejection based on cost-sensitive classiÔ¨Åcation. In Marina Meila and Tong Zhang (eds.), Pro- ceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 1507‚Äì1517. PMLR, 18‚Äì24 Jul 2021. C. Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on Information Theory, 16(1):41‚Äì46, 1970. doi: 10.1109/TIT.1970.1054406. Corinna Cortes, Giulia De Salvo, and Mehryar Mohri. Learning with rejection. In ALT, 2016. Corinna Cortes, Giulia De Salvo, and Mehryar Mohri. Theory and algorithms for learning with rejection in binary classiÔ¨Åcation. Annals of Mathematics and ArtiÔ¨Åcial Intelligence, 2023. Andrew Cotter, Maya Gupta, and Harikrishna Narasimhan. On making stochastic classiÔ¨Åers deter- ministic. Advances in Neural Information Processing Systems, 32, 2019a. Andrew Cotter, Heinrich Jiang, Maya Gupta, Serena Wang, Taman Narayan, Seungil You, and Karthik Sridharan. Optimization with non-differentiable constraints with applications to fairness, recall, churn, and other goals. Journal of Machine Learning Research, 20(172):1‚Äì59, 2019b. URL http://jmlr.org/papers/v20/18-616.html. Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of samples. In CVPR, 2019. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248‚Äì255. Ieee, 2009. Zongyong Deng, Hao Liu, Yaoxing Wang, Chenyang Wang, Zekuan Yu, and Xuehong Sun. PML: progressive margin loss for long-tailed age classiÔ¨Åcation. Co RR, abs/2103.02140, 2021. URL https://arxiv.org/abs/2103.02140. Ran El-Yaniv and Yair Wiener. On the foundations of noise-free selective classiÔ¨Åcation. Journal of Machine Learning Research, 11(53):1605‚Äì1641, 2010. URL http://jmlr.org/papers/ v11/el-yaniv10a.html. Aditya Gangrade, Anil Kag, and Venkatesh Saligrama. Selective classiÔ¨Åcation via one-sided prediction. In Arindam Banerjee and Kenji Fukumizu (eds.), Proceedings of The 24th In- ternational Conference on ArtiÔ¨Åcial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pp. 2179‚Äì2187. PMLR, 13‚Äì15 Apr 2021. URL https: //proceedings.mlr.press/v130/gangrade21a.html. 10
Published as a conference paper at ICLR 2024 Yonatan Geifman and Ran El-Yaniv. Selective classiÔ¨Åcation for deep neural networks. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS‚Äô17, pp. 4885‚Äì4894, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964. Haibo He and Edwardo A. Garcia. Learning from imbalanced data. IEEE Transactions on Knowledge and Data Engineering, 21(9):1263‚Äì1284, 2009. Muhammad Abdullah Jamal, Matthew Brown, Ming-Hsuan Yang, Liqiang Wang, and Boqing Gong. Rethinking class-balanced methods for long-tailed visual recognition from a domain adaptation perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. Erik Jones, Shiori Sagawa, Pang Wei Koh, Ananya Kumar, and Percy Liang. Selective classi- Ô¨Åcation can magnify disparities across groups. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. Open Review.net, 2021. URL https://openreview.net/forum?id=N0M_4Bk Q05i. Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yan- nis Kalantidis. Decoupling representation and classiÔ¨Åer for long-tailed recognition. In Eighth International Conference on Learning Representations (ICLR), 2020. Ganesh Ramachandra Kini, Orestis Paraskevas, Samet Oymak, and Christos Thrampoulidis. Label- imbalanced and group-sensitive classiÔ¨Åcation under overparameterization. Co RR, abs/2103.01550, 2021. URL https://arxiv.org/abs/2103.01550. Oluwasanmi O Koyejo, Nagarajan Natarajan, Pradeep K Ravikumar, and Inderjit S Dhillon. Con- sistent binary classiÔ¨Åcation with generalized performance metrics. In NIPS, pp. 2744‚Äì2752, 2014. Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. Joshua K Lee, Yuheng Bu, Deepta Rajan, Prasanna Sattigeri, Rameswar Panda, Subhro Das, and Gregory W Wornell. Fair selective classiÔ¨Åcation via sufÔ¨Åciency. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 6076‚Äì6086. PMLR, 18‚Äì24 Jul 2021. URL https://proceedings.mlr.press/v139/lee21b.html. Zachary C Lipton, Charles Elkan, and Balakrishnan Naryanaswamy. Optimal thresholding of classiÔ¨Åers to maximize f1 measure. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part II 14, pp. 225‚Äì239. Springer, 2014. Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X. Yu. Large- scale long-tailed recognition in an open world. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. Anqi Mao, Christopher Mohri, Mehryar Mohri, and Yutao Zhong. Two-stage learning to defer with multiple experts. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. URL https://openreview.net/forum?id=GIls H0T4b2. Anqi Mao, Mehryar Mohri, and Yutao Zhong. Ranking with abstention. ar Xiv preprint ar Xiv:2307.02035, 2023b. Aditya K Menon, Ankit Singh Rawat, Sashank Reddi, Seungyeon Kim, and Sanjiv Kumar. A statistical perspective on distillation. In International Conference on Machine Learning, pp. 7632‚Äì7642. PMLR, 2021a. Aditya Krishna Menon, Harikrishna Narasimhan, Shivani Agarwal, and Sanjay Chawla. On the statistical consistency of algorithms for binary classiÔ¨Åcation under class imbalance. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pp. 603‚Äì611, 2013. 11
Published as a conference paper at ICLR 2024 Aditya Krishna Menon, Ankit Singh Rawat, and Sanjiv Kumar. Overparameterisation and worst-case generalisation: friend or foe? In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. Open Review.net, 2021b. URL https: //openreview.net/forum?id=jphn JNOwe36. Jooyoung Moon, Jihyo Kim, Younghak Shin, and Sangheum Hwang. ConÔ¨Ådence-aware learning for deep neural networks. In international conference on machine learning, pp. 7034‚Äì7044. PMLR, 2020. Hussein Mozannar and David Sontag. Consistent estimators for learning to defer to an expert. In Hal Daum√© III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 7076‚Äì7087. PMLR, 13‚Äì18 Jul 2020. Harikrishna Narasimhan. Learning with complex loss functions and constraints. In International Conference on ArtiÔ¨Åcial Intelligence and Statistics, pp. 1646‚Äì1654, 2018. Harikrishna Narasimhan and Aditya K Menon. Training over-parameterized models with non- decomposable objectives. Advances in Neural Information Processing Systems, 34:18165‚Äì18181, 2021. Harikrishna Narasimhan, Harish Ramaswamy, Aadirupa Saha, and Shivani Agarwal. Consistent multiclass algorithms for complex performance measures. In ICML, pp. 2398‚Äì2407, 2015a. Harikrishna Narasimhan, Harish Ramaswamy, Aadirupa Saha, and Shivani Agarwal. Consistent multiclass algorithms for complex performance measures. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 2398‚Äì2407, Lille, France, 07‚Äì09 Jul 2015b. PMLR. Harikrishna Narasimhan, Andrew Cotter, and Maya Gupta. Optimizing generalized rate metrics with three players. In Advances in Neural Information Processing Systems, pp. 10746‚Äì10757, 2019. Harikrishna Narasimhan, Wittawat Jitkrittum, Aditya K Menon, Ankit Rawat, and Sanjiv Ku- mar. Post-hoc estimators for learning to defer to an expert. In S. Koyejo, S. Mo- hamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural In- formation Processing Systems, volume 35, pp. 29292‚Äì29304. Curran Associates, Inc., 2022a. URL https://proceedings.neurips.cc/paper_files/paper/2022/ file/bc8f76d9caadd48f77025b1c889d2e2d-Paper-Conference.pdf. Harikrishna Narasimhan, Harish G Ramaswamy, Shiv Kumar Tavker, Drona Khurana, Praneeth Netrapalli, and Shivani Agarwal. Consistent multiclass algorithms for complex metrics and constraints. ar Xiv preprint ar Xiv:2210.09695, 2022b. Jerzy Neyman and Egon Sharpe Pearson. Ix. on the problem of the most efÔ¨Åcient tests of statistical hypotheses. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 231(694-706):289‚Äì337, 1933. Chenri Ni, Nontawat Charoenphakdee, Junya Honda, and Masashi Sugiyama. On the calibration of multiclass classiÔ¨Åcation with rejection. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d‚ÄôAlch√©-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, Neur IPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 2582‚Äì2592, 2019. Harish G. Ramaswamy, Ambuj Tewari, and Shivani Agarwal. Consistent algorithms for multiclass classiÔ¨Åcation with an abstain option. Electronic Journal of Statistics, 12(1):530 ‚Äì 554, 2018. doi: 10.1214/17-EJS1388. Jiawei Ren, Cunjun Yu, shunan sheng, Xiao Ma, Haiyu Zhao, Shuai Yi, and hongsheng Li. Balanced meta-softmax for long-tailed visual recognition. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 4175‚Äì4186. Curran Associates, Inc., 2020. 12
Published as a conference paper at ICLR 2024 S. Sagawa, P. W. Koh, T. B. Hashimoto, and P. Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. In International Conference on Learning Representations (ICLR), 2020. Carla M. Santos-Pereira and Ana M. Pires. On optimal reject rules and ROC curves. Pattern Recognition Letters, 26(7):943‚Äì952, 2005. ISSN 0167-8655. doi: https://doi.org/10.1016/j. patrec.2004.09.042. URL https://www.sciencedirect.com/science/article/ pii/S0167865504002892. Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and Trends¬Æ in Machine Learning, 4(2):107‚Äì194, 2012. Song-Qing Shen, Bin-Bin Yang, and Wei Gao. AUC optimization with a reject option. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 34, pp. 5684‚Äì5691, 2020. J. Tan, C. Wang, B. Li, Q. Li, W. Ouyang, C. Yin, and J. Yan. Equalization loss for long-tailed object recognition. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11659‚Äì11668, 2020. S. K. Tavker, H. G. Ramaswamy, and H. Narasimhan. Consistent plug-in classiÔ¨Åers for complex objectives and constraints. In Advances in Neural Information Processing Systems, 2020. Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classiÔ¨Åcation and detection dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, 2018. Rajeev Verma and Eric Nalisnick. Calibrated learning to defer with one-vs-all classiÔ¨Åers. ar Xiv preprint ar Xiv:2202.03673, 2022. Jiaqi Wang, Wenwei Zhang, Yuhang Zang, Yuhang Cao, Jiangmiao Pang, Tao Gong, Kai Chen, Ziwei Liu, Chen Change Loy, and Dahua Lin. Seesaw loss for long-tailed instance segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2021. Serena Wang, Harikrishna Narasimhan, Yichen Zhou, Sara Hooker, Michal Lukasik, and Aditya Kr- ishna Menon. Robust distillation for worst-class performance: on the interplay between teacher and student objectives. In Uncertainty in ArtiÔ¨Åcial Intelligence, pp. 2237‚Äì2247. PMLR, 2023. Tong Wu, Qingqiu Huang, Ziwei Liu, Yu Wang, and Dahua Lin. Distribution-balanced loss for multi- label classiÔ¨Åcation in long-tailed datasets. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision ‚Äì ECCV 2020, pp. 162‚Äì178, Cham, 2020a. Springer International Publishing. ISBN 978-3-030-58548-8. Tz-Ying Wu, Pedro Morgado, Pei Wang, Chih-Hui Ho, and Nuno Vasconcelos. Solving long-tailed recognition with deep realistic taxonomic classiÔ¨Åer. In Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part VIII 16, pp. 171‚Äì189. Springer, 2020b. Bowei Yan, Sanmi Koyejo, Kai Zhong, and Pradeep Ravikumar. Binary classiÔ¨Åcation with karmic, threshold-quasi-concave metrics. In International Conference on Machine Learning, pp. 5531‚Äì 5540. PMLR, 2018. Forest Yang, Moustapha Cisse, and Sanmi Koyejo. Fairness with overlapping groups. In Neur IPS, 2020. Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature transfer learning for face recognition with under-represented data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. Junjie Zhang, Lingqiao Liu, Peng Wang, and Chunhua Shen. To balance or not to balance: A simple-yet-effective approach for learning with long-tailed distributions, 2019. Songyang Zhang, Zeming Li, Shipeng Yan, Xuming He, and Jian Sun. Distribution alignment: A uniÔ¨Åed framework for long-tail visual recognition. In CVPR, 2021. 13
Published as a conference paper at ICLR 2024 Appendix A PROOFS FOR RESULTS IN MAIN TEXT A.1 PROOF OF THEOREM 1 The proof follows as a corollary of Theorem 12 in Appendix D.3 by setting Œ≤k = 1 K , ‚àÄk, and scaling the resulting parameters Œ±‚àó y by K. Note that Theorem 12 considers a minimization problem that allows for stochastic classiÔ¨Åer-rejector mechanisms. However, since the optimal solution is a deterministic classiÔ¨Åer-rejector pair, it still remains the optimal solution when the minimization is over only deterministic classiÔ¨Åer-rejector pairs. A.2 PROOF OF LEMMA 2 We prove a more general version of Lemma 2. Consider a variation of Chow‚Äôs rule where we modify the base model training to optimize a weighted version of the CE loss in Eq. (3): X k‚àà[K] Œ≤k ¬∑ E [1(y ‚ààGk) ¬∑ ‚Ñìce(y, f(x))] , (15) for some Œ≤ ‚ààRK + . One may then use the class probability estimates pwt y (x) ‚àùexp(f wt y (x)) from the learned scorer f wt to implement Chow‚Äôs rule: hwt(x) ‚ààarg maxy‚àà[L] pwt y (x); rwt(x) = 1 ‚áê‚áímaxy‚àà[L] pwt y (x) < 1 ‚àíc. (16) Lemma 5. The variant of Chow‚Äôs rule in Eq. (16), where the base model is trained with the balanced cross-entropy loss, results in a classiÔ¨Åer and rejector of the form: hwt(x) = arg max y‚àà[L] Œ≤[y] ¬∑ Œ∑y(x) rbal(x) = 1 ‚áê‚áí max y‚àà[L] Œ≤[y] ¬∑ Œ∑y(x) <  X y‚Ä≤‚àà[L] Œ≤[y‚Ä≤] ¬∑ Œ∑y‚Ä≤(x)  ¬∑ (1 ‚àíc), (17) where [y] is the index of the group to which class y belongs. Proof. We re-write the weighted CE loss in Eq. (15) as: X y‚àà[L] E  Œ≤[y] ¬∑ ‚Ñìce(y, f(x))  . Since the softmax CE loss is a strictly proper-composite loss, we have that the minimizer pwt y (x) ‚àù exp(f wt y (x)) of this objective takes the form: pwt y (x) = 1 Z(x) ¬∑ Œ≤[y] ¬∑ Œ∑y(x), where Z(x) = P y‚Ä≤ Œ≤[y‚Ä≤] ¬∑ Œ∑y‚Ä≤(x). When applying Chow‚Äôs rule to these probabilities, we get: rwt(x) = 1 ‚áê‚áí max y pwt y (x) < 1 ‚àíc, which is the same as: rwt(x) = 1 ‚áê‚áí max y Œ≤[y] ¬∑ Œ∑y(x) < Ô£´ Ô£≠X y‚Ä≤ Œ≤[y‚Ä≤] ¬∑ Œ∑y‚Ä≤(x) Ô£∂ Ô£∏¬∑ (1 ‚àíc), as desired. Proof of Lemma 2. The proof follows from Lemma 5 by setting Œ≤k = 1 œÄk . 14
Published as a conference paper at ICLR 2024 A.3 PROOF OF THEOREM 3 See Appendix D.5. A.4 PROOF OF THEOREM 4 The proof follows from an application of the standard convergence guarantees for the exponentiated gradient descent algorithm (Shalev-Shwartz et al., 2012). One can further bound the estimation errors œµgen t in each iteration of Algorithm 2 using standard generalization analysis (Narasimhan et al., 2015a; Tavker et al., 2020; Wang et al., 2023), noting that the classiÔ¨Åer and rejectors (ht, rt) are plug-in estimators chosen from a Ô¨Ånite capacity class (namely that of post-hoc linear adjustments to a Ô¨Åxed model p). Proof. Recall that the worst-group L2R risk is given by: Rrej wst(h, r) = max k ek(h, r) + c ¬∑ P(r(x) = 1). This risk can be equivalently re-written as: Rrej wst(h, r) = max Œ≤‚àà‚àÜK X k Œ≤k ¬∑ ek(h, r) + c ¬∑ P(r(x) = 1). We apply standard guarantees for the exponentiated gradient ascent algorithm (Shalev-Shwartz et al., 2012) to treating ÀÜe1(h(t), r(t)), . . . , ÀÜe K(h(t), r(t)) as the losses incurred by K experts. We have that when Œæ = q log(K) T , the iterates (h(1), r(1)), . . . , (h(T ), r(T )) in Algorithm 2 satisfy: max Œ≤‚àà‚àÜK 1 T T X t=1 X k Œ≤k ¬∑ ÀÜek(h(t), r(t)) ‚â§1 T T X t=1 X k Œ≤(t) k ¬∑ ÀÜek(h(t), r(t)) + 2 r log(K) T . Bounding by the estimation error in each iteration, we get: max Œ≤‚àà‚àÜK 1 T T X t=1 X k Œ≤k ¬∑ (ek(h(t), r(t)) ‚àíœµgen t ) ‚â§1 T T X t=1 X k Œ≤(t) k ¬∑ (ek(h(t), r(t)) + œµgen t ) + 2 r log(K) T , which is the same as: max Œ≤‚àà‚àÜK 1 T T X t=1 X k Œ≤k ¬∑ ek(h(t), r(t)) ‚â§1 T T X t=1 X k Œ≤(t) k ¬∑ ek(h(t), r(t)) + 2¬Øœµgen + 2 r log(K) T . Adding c ¬∑ P(r(t)(x) = 1) on either side, we have: max Œ≤‚àà‚àÜK 1 T T X t=1 X k Œ≤k ¬∑ ek(h(t), r(t)) + c ¬∑ P(r(t)(x) = 1) ‚â§1 T T X t=1 X k Œ≤(t) k ¬∑ ek(h(t), r(t)) + c ¬∑ P(r(t)(x) = 1) + 2¬Øœµgen + 2 r log(K) T ‚â§1 T T X t=1 inf h,r (X k Œ≤(t) k ¬∑ ek(h, r) + c ¬∑ P(r(x) = 1) ) + 1 T X t œµcs t + 2¬Øœµgen + 2 r log(K) T (18) ‚â§inf h,r 1 T T X t=1 X k Œ≤(t) k ¬∑ ek(h, r) + c ¬∑ P(r(x) = 1) + ¬Øœµcs + 2¬Øœµgen + 2 r log(K) T ‚â§inf h,r X k ¬ØŒ≤k ¬∑ ek(h, r) + c ¬∑ P(r(x) = 1) + ¬Øœµcs + 2¬Øœµgen + 2 r log(K) T (19) 15
Published as a conference paper at ICLR 2024 = inf h,r max Œ≤‚àà‚àÜK X k Œ≤k ¬∑ ek(h, r) + c ¬∑ P(r(x) = 1) + ¬Øœµcs + 2¬Øœµgen + 2 r log(K) T ‚â§inf h,r max k ek(h, r) + c ¬∑ P(r(x) = 1) + ¬Øœµcs + 2¬Øœµgen + 2 r log(K) T = inf h,r Rrej wst(h, r) + ¬Øœµcs + 2¬Øœµgen + 2 r log(K) T , (20) where in Eq. (18), we apply the bound on the excess cost-sensitive risk in line 4 of the algorithm, and in Eq. (19), we denote ¬ØŒ≤k = 1 T PT t=1 Œ≤(t) k . We can re-write the LHS as: max Œ≤‚àà‚àÜK 1 T T X t=1 X k Œ≤k ¬∑  ek(h(t), r(t)) + c ¬∑ P(r(t) = 1)  = max Œ≤‚àà‚àÜK X k Œ≤k ¬∑ Et‚àºU(1,...,T ) h ek(h(t), r(t)) + c ¬∑ P(r(t) = 1) i = max k‚àà[K] Et‚àºU(1,...,T ) h ek(h(t), r(t)) + c ¬∑ P(r(t) = 1) i = max k‚àà[K] Et‚àºU(1,...,T ) h ek(h(t), r(t)) i + c ¬∑ Et‚àºU(1,...,T ) h P(r(t) = 1) i Substituting back into Eq. (20) completes the proof. B EQUIVALENCE BETWEEN SELECTIVE CLASSIFICATION AND L2R Selective classiÔ¨Åcation seeks to learn a classiÔ¨Åer and rejector that minimizes the misclassiÔ¨Åcation error with a constraint on the proportion of abstentions: min h,r P (y Ã∏= h(x), r(x) = 0) s.t. P (r(x) = 1) ‚â§b, (21) where b > 0 is an abstention budget. Theorem 6. Suppose P(y | x) and P(x) are continuous in x. For any such abstention budget b > 0 in the constrained problem in Eq. (21), there exists a cost c ‚â•0 cost such that the minimizer of the L2R risk below is also a minimizer for Eq. (21): P (y Ã∏= h(x), r(x) = 0) + c ¬∑ P (r(x) = 1) . The Lagrangian for the constrained problem in Eq. (21) is given by: L(h, r, Œª) = P (y Ã∏= h(x), r(x) = 0) + Œª ¬∑ (P(r(x) = 1) ‚àíb) , where Œª ‚â•0 is the Lagrange multiplier. We will Ô¨Ånd it useful to state the following lemma: Lemma 7. Let (h‚àó Œª, r‚àó Œª) be the minimizer of L(h, r, Œª) for Lagrange multiplier Œª ‚â•0. Then: P (y Ã∏= h‚àó Œª(x), r‚àó Œª(x) = 0) ‚â§P (y Ã∏= h(x), r(x) = 0) , for all (h, r) such that P(r(x) = 1) ‚â§P(r‚àó Œª(x) = 1). Proof. Since (h‚àó Œª, r‚àó Œª) minimizes the Lagrangian, for any (h, r), L(h‚àó Œª, r‚àó Œª, Œª) ‚â§L(h, r, Œª), i.e., P (y Ã∏= h‚àó Œª(x), r‚àó Œª(x) = 0) ‚â§P (y Ã∏= h(x), r(x) = 0) + Œª ¬∑ (P(r(x) = 1) ‚àíP(r‚àó Œª(x) = 1)) . Since Œª ‚â•0, for any (h, r) such that P(r(x) = 1) ‚â§P(r‚àó Œª(x) = 1), P (y Ã∏= h‚àó Œª(x), r‚àó Œª(x) = 0) ‚â§P (y Ã∏= h(x), r(x) = 0) , as desired. We are now ready to prove Theorem 6. 16
Published as a conference paper at ICLR 2024 Proof of Theorem 6. For a Ô¨Åxed Œª ‚â•0, the following is a minimizer of the Lagrangian L(h, r, Œª): h‚àó Œª(x) ‚ààarg max y‚àà[L] P(y | x); r‚àó Œª(x) = 1 ‚áê‚áí max y‚àà[L] P(y | x) < 1 ‚àíŒª. The abstention rate for r‚àó Œª(x) can then be written as: P(r‚àó Œª(x) = 1) = Z maxy‚àà[L] P(y|x)<1‚àíŒª P(x) dx. Since P(y | x) and P(x) are continuous in x, we can always Ô¨Ånd a Œª ‚â•0 for which P(r‚àó Œª(x) = 1) = b. Applying Lemma 7 with this choice of Œª, we then have that P (y Ã∏= h‚àó Œª(x), r‚àó Œª(x) = 0) ‚â§P (y Ã∏= h(x), r(x) = 0) , for all (h, r) such that P(r(x) = 1) ‚â§b. Setting c = Œª completes the proof. C CONDITIONAL LEARNING TO REJECT Geifman & El-Yaniv (2017) minimize a variant of the L2R risk in Eq. (1), where the misclassiÔ¨Åcation error is conditioned on samples that are not rejected: Rrej cond(h, r) = P (y Ã∏= h(x) | r(x) = 0) + c ¬∑ P (r(x) = 1) . (22) We show below that the optimal classiÔ¨Åer and rejector for this conditional L2R risk continues to have the same form as Eq. (2), but uses a different (distribution-dependent) rejection threshold: Lemma 8. Under Assumption 1, the Bayes-optimal solution to Eq. (22) is given by: h‚àó(x) = argmax y‚àà[L] Œ∑y(x); r‚àó(x) = 1 ‚áê‚áí max y‚àà[L] Œ∑y(x) < 1 ‚àíc‚Ä≤, (23) for some distribution-dependent threshold c‚Ä≤ ‚àà[0, 1]. Proof. The conditional L2R risk in Eq. (22) can be expressed as: Rrej cond(h, r) = P (y Ã∏= h(x), r(x) = 0) 1 ‚àíP (r(x) = 1) + c ¬∑ P (r(x) = 1) . Minimizing this risk can be posed as an equivalent constrained optimization problem: min h,r,Œ±‚àà(0,1] 1 Œ± ¬∑ P (y Ã∏= h(x), r(x) = 0) + c ¬∑ P (r(x) = 1) s.t. P (r(x) = 1) = 1 ‚àíŒ±. In particular, there exists an Œ±‚àó‚àà(0, 1], such that the minimizer of the following constrained problem also minimizes Eq. (22): min h,r 1 Œ±‚àó¬∑ P (y Ã∏= h(x), r(x) = 0) + c ¬∑ P (r(x) = 1) s.t. P (r(x) = 1) = 1 ‚àíŒ±‚àó (24) Using arguments similar to the Neyman-Pearson lemma (Neyman & Pearson, 1933), one can show that, under Assumption 1, there exists a Lagrange multiplier ¬µ‚àó‚ààR such that the minimizer to the following problem is also a minimizer to Eq. (24): min h,r 1 Œ±‚àó¬∑ P (y Ã∏= h(x), r(x) = 0) + (c + ¬µ‚àó) ¬∑ P (r(x) = 1) , or equivalently: min h,r Ex  1 Œ±‚àó¬∑ (1 ‚àíŒ∑h(x)(x)) ¬∑ 1 (r(x) = 0) + (c + ¬µ‚àó) ¬∑ 1 (r(x) = 1)  . The following are then minimizers of the above objective: h‚àó(x) ‚ààarg max y‚àà[L] Œ∑y(x) and r‚àó(x) = 1 ‚áê‚áí max y‚àà[L] Œ∑y(x) < 1 ‚àíŒ±‚àó¬∑ (c + ¬µ‚àó). Setting c‚Ä≤ = Œ±‚àó¬∑ (c + ¬µ‚àó) completes the proof. 17
Published as a conference paper at ICLR 2024 D BAYES-OPTIMAL CLASSIFIERS AND REJECTORS We derive the Bayes-optimal classiÔ¨Åer and rejector for the balanced L2R risk (6) and the generalized L2R risk (14). We will will Ô¨Årst consider a general formulation where we allow stochastic classiÔ¨Åer- rejector mechanisms and then specialize to the case of deterministic classiÔ¨Åers and rejectors. D.1 CONFUSION STATISTICS We begin with some useful deÔ¨Ånitions. Suppose Y = [n] = {1, . . . , n}. We deÔ¨Åne the confusion matrix for a classiÔ¨Åer-rejector pair (h, r) by: Cij(h, r) = E(x,y)‚àºP [1 (y = i, h(x) = j, r(x) = 0)] We allow for stochastic mechanisms F that are deÔ¨Åned by a distribution over classiÔ¨Åer-rejector pairs. We adopt the convention in Narasimhan et al. (2015a) and deÔ¨Åne the confusion matrix for a stochastic mechanism F by: Cij(F) = E(h,r)‚àºF [Cij(h, r)] . We will also Ô¨Ånd it useful to deÔ¨Åne the space of all confusion matrices realizable by distributions over classiÔ¨Åers-rejector pairs: C =  C(F) ‚àà[0, 1]n√ón ‚àÄdistributions F over (h, r) . It is easy to show that the set C is convex. Lemma 9. C is a convex set. Proof. For any C(1), C(2) ‚ààC and Œ≥ ‚àà[0, 1], we will show Œ≥ ¬∑ C(1) + (1 ‚àíŒ≥) ¬∑ C(2) ‚ààC. Clearly, there exists stochastic mechanisms F1, F2 such that C(1) = C(F1) and C(2) = C(F2). Since F ‚Ä≤ = Œ≥¬∑F1+(1‚àíŒ≥)¬∑F2 represents a valid stochastic mechanism, C(F ‚Ä≤) = Œ≥¬∑C(1)+(1‚àíŒ≥)¬∑C(2) ‚ààC. D.2 OPTIMAL SOLUTIONS TO LINEAR MINIMIZATION OVER C We Ô¨Årst state a couple of helper lemmas that characterize the optimal solutions to linear minimization problems over the set C. Suppose we wish to minimize the following objective over stochastic mechanisms F: min F X ij Wij ¬∑ Cij[F], for coefÔ¨Åcients W ‚ààRL√óL. One may equivalently formulate this as a linear minimization over C: min C‚ààC X ij Wij ¬∑ Cij. (25) Lemma 10. Suppose Assumption 1 holds. Let W ‚ààRL√óL be a coefÔ¨Åcient matrix such that no two columns are identical. Then Eq. (25) admits a unique solution. Furthermore, the minimizing confusion matrix is achieved by the following deterministic classiÔ¨Åer-rejector pair: h‚àó(x) ‚ààarg min y‚àà[L] X i Wi,y ¬∑ Œ∑i(x); r‚àó(x) = 1 ‚áê‚áí min y‚àà[L] X i Wi,y ¬∑ Œ∑i(x) > 0. Proof. The Ô¨Årst part (uniqueness claim) directly follows from Lemma 24 in Narasimhan et al. (2022b). For the second part, we re-write Eq. (25) into an minimization problem over stochastic mechanisms F: min C‚ààC X i,j Wij ¬∑ Cij = min F X i,j Wij ¬∑ Cij(F) = min F E(h,r)‚àºF h X i,j Wij ¬∑ Cij(h, r) i 18
Published as a conference paper at ICLR 2024 = min (h,r) X i,j Wij ¬∑ Cij(h, r) = min (h,r) Ex h X i,j Wij ¬∑ P(y = i, h(x) = j, r(x) = 0) i = min (h,r) Ex h X i,j Wij ¬∑ Œ∑i(x) ¬∑ 1(h(x) = j, r(x) = 0) i , (26) where the third step follows from the fact that for any function œï, the expectation E(h,r)‚àºF [œï(h, r)] is minimized by a point mass on arg min(h,r) œï(h, r). The minimizer of Eq. (26) minimizes the term within the expectation pointwise: for any given x, (h‚àó(x), r‚àó(x)) ‚àà arg min y‚àà[L],z‚àà{0,1} X i Wiy ¬∑ Œ∑i(x) ¬∑ 1(z = 0), which takes the form in the statement of the lemma. For our next helper lemma, we consider a constrained minimization problem over stochastic mecha- nisms F: min F X i,j Wij ¬∑ Cij[F] s.t. X i,j w(k) i ¬∑ Cij[F] = bk, ‚àÄk ‚àà[K], for coefÔ¨Åcients W ‚ààRL√óL and w(1), . . . , w(K) ‚ààRL. Once again, we may formulate this as an equivalent constrained linear minimization over C: min C‚ààC X i,j Wij ¬∑ Cij s.t. X i,j w(k) i ¬∑ Cij = bk, ‚àÄk ‚àà[K]. (27) Lemma 11. Suppose Assumption 1 holds. Let W ‚ààRL√óL be a coefÔ¨Åcient matrix such that no two columns are identical. Let w(1), . . . , w(K) ‚ààRL be constraint coefÔ¨Åcients so that Eq. (27) admits a feasible solution. Then there exists multipliers ¬µ‚àó‚ààRK such that the minimizer to Eq. (27) coincides with the minimizer to the following unconstrained linear minimization over C: min C‚ààC X i,j  Wij ‚àí X k‚àà[K] ¬µ‚àó k ¬∑ w(k) i  ¬∑ Cij. (28) Proof. We Ô¨Årst write the Lagrangian for Eq. (27): L(C, ¬µ) = X i,j Wij ¬∑ Cij ‚àí X k‚àà[K] ¬µk ¬∑  X i,j w(k) i ¬∑ Cij ‚àíbk  , (29) where ¬µ1, . . . , ¬µK ‚ààR are multipliers associated with the K constraints. Since the objective and constraints in Eq. (27) are linear in C, and C is convex from Lemma 9, Eq. (27) is a convex minimization problem. Furthermore, since it has at least one feasible solution, Slater‚Äôs condition is satisÔ¨Åed, and strong duality holds. As a result, there exists an optimal primal solution C‚àóthat satisÔ¨Åes the constraints in Eq. (27), and optimal dual solution ¬µ‚àósuch that: L(C‚àó, ¬µ‚àó) = min C‚ààC L(C, ¬µ‚àó) = max ¬µ‚ààRK L(C‚àó, ¬µ) = X i,j Wij ¬∑ C‚àó ij. (30) Furthermore, we have that: min C‚ààC L(C, ¬µ‚àó) = min C‚ààC X i,j  Wij ‚àí X k‚àà[K] ¬µ‚àó k ¬∑ w(k) i  ¬∑ Cij + œâ‚àó= min C‚ààC X i,j Lij ¬∑ Cij + œâ‚àó, where Lij = Wij ‚àíP k‚àà[K] ¬µ‚àó k ¬∑ w(k) i and œâ‚àó= P k‚àà[K] ¬µ‚àó k ¬∑ bk is independent of C. Observe that the coefÔ¨Åcient matrix L on the right-hand side has no two columns identical. To see this, each column of L is of the form L:,j = W:,j ‚àíP k‚àà[K] ¬µ‚àó k ¬∑ w(k), where w(k) = [w(k) 1 , . . . , w(k) L ]‚ä§. Since W has no two columns identical, i.e., W:j Ã∏= W:j‚Ä≤, ‚àÄj Ã∏= j‚Ä≤, we have that L:,j Ã∏= L:,j‚Ä≤. Using Assumption 1 and the above observation, we can apply the Ô¨Årst part of Lemma 10 to show that min C‚ààC L(C, ¬µ‚àó) has a unique solution. Since L(C‚àó, ¬µ‚àó) = min C‚ààC L(C, ¬µ‚àó) from Eq. (30), we have that the optimal primal solution C‚àóis a unique minimizer to min C‚ààC L(C, ¬µ‚àó). In other words, C‚àóis the unique minimizer to the unconstrained linear problem in Eq. (28). 19
Published as a conference paper at ICLR 2024 D.3 BAYES-OPTIMAL CLASSIFIER AND REJECTOR FOR COST-SENSITIVE L2R In this section, we derive the Bayes-optimal stochastic mechanism that minimizes the following cost-sensitive objective, with a penalty of c > 0 for abstention. SpeciÔ¨Åcally, deÔ¨Åne: Rrej cs (h, r) = X k‚àà[K] Œ≤k ¬∑ P  y Ã∏= h(x) r(x) = 0, y ‚ààGk  + c ¬∑ P (r(x) = 1) , (31) where Œ≤ ‚àà‚àÜK are the costs associated with the K groups. One can re-deÔ¨Åne Eq. (31) in terms of the confusion matrices C(h, r) as follows: Rrej cs (h, r) = X k‚àà[K] Œ≤k ¬∑ P  y Ã∏= h(x), r(x) = 0, y ‚ààGk  P  r(x) = 0, y ‚ààGk  + c ¬∑ P (r(x) = 1) = X k‚àà[K] Œ≤k ¬∑ P i‚ààGk P jÃ∏=i Cij(h, r) P i‚ààGk P j‚àà[L] Cij(h, r) + c ¬∑  1 ‚àí X i,j Cij(h, r)  . For a stochastic mechanism F, we can similarly deÔ¨Åne Rrej cs (F) = X k‚àà[K] Œ≤k ¬∑ P i‚ààGk P jÃ∏=i Cij(F) P i‚ààGk P j‚àà[L] Cij(F) + c ¬∑  1 ‚àí X i,j Cij(F)  . (32) We derive an optimal stochastic classiÔ¨Åer that minimizes Eq. (32): Theorem 12. Under Assumptions 1 and 2, there exists group-speciÔ¨Åc parameters Œ±‚àó‚àà(0, 1)K and ¬µ‚àó‚ààRK such that the following deterministic classiÔ¨Åer-rejector pair is an optimal solution to Eq. (32): h‚àó(x) = arg max y‚àà[L] Œ≤[y] Œ±‚àó [y] ¬∑ Œ∑y(x); r‚àó(x) = 1 ‚áê‚áí max y‚àà[L] Œ≤[y] Œ±‚àó [y] ¬∑ Œ∑y(x) < X y‚Ä≤‚àà[L] Œ≤[y‚Ä≤] Œ±‚àó [y‚Ä≤] ‚àí¬µ‚àó [y‚Ä≤] ! ¬∑ Œ∑y‚Ä≤(x) ‚àíc, (33) where [y] is the index of the group class y belongs to. Furthermore, Œ±‚àó k = P (r‚àó(x) = 0, y ‚ààGk). Proof. From the boundedness condition in Assumption 2, we know that there exists Œ±‚àó‚àà(0, 1)K such that the minimizer to the following problem also minimizes Eq. (32): min F X k‚àà[K] Œ≤k Œ±‚àó k ¬∑ X i‚ààGk X jÃ∏=i Cij(F) + c ¬∑  1 ‚àí X i,j Cij(F)  (34) s.t. X i‚ààGk X j‚àà[L] Cij(F) = Œ±‚àó k, ‚àÄk ‚àà[K]. One may equivalently formulate a constrained linear minimization problem over C: min C‚ààC X k‚àà[K] Œ≤k Œ±‚àó k ¬∑ X i‚ààGk X jÃ∏=i Cij + c ¬∑  1 ‚àí X i,j Cij  (35) s.t. X i‚ààGk X j‚àà[L] Cij = Œ±‚àó k, ‚àÄk ‚àà[K]. One can now directly apply the result in Lemma 11 with Wij = P k‚àà[K] Œ≤k Œ±‚àó k ¬∑ 1(i ‚ààGk, j Ã∏= i) ‚àíc and w(k) i = 1(i ‚ààGk), ‚àÄi ‚àà[L], noting that no two columns of W are identical. Consequently, we have that there exists multipliers ¬µ‚àó‚ààRK such that the minimizer to the following unconstrained linear minimization over C also minimizes Eq. (35): min C‚ààC X i,j  Wij ‚àí X k‚àà[K] ¬µ‚àó k ¬∑ w(k) i  ¬∑ Cij. 20
Published as a conference paper at ICLR 2024 Applying Lemma 10, we have that the above problem admits a unique minimizer C‚àówhich is achieved by the following deterministic classiÔ¨Åer-rejector pair: h‚àó(x) ‚ààarg min y‚àà[L] X i  Wiy ‚àí X k‚àà[K] ¬µ‚àó k ¬∑ w(k) i  ¬∑ Œ∑i(x); r‚àó(x) = 1 ‚áê‚áí min y‚àà[L] X i  Wiy ‚àí X k‚àà[K] ¬µ‚àó k ¬∑ w(k) i  ¬∑ Œ∑i(x) > 0. Substituting for W and w(k) i then gives us: h‚àó(x) ‚ààarg min y‚àà[L] X k‚àà[K] X i‚ààGk  Œ≤k Œ±‚àó k ¬∑ 1(y Ã∏= i) ‚àí¬µ‚àó k  ¬∑ Œ∑i(x) ‚àíc; r‚àó(x) = 1 ‚áê‚áí min y‚àà[L] X k‚àà[K] X i‚ààGk  Œ≤k Œ±‚àó k ¬∑ 1(y Ã∏= i) ‚àí¬µ‚àó k  ¬∑ Œ∑i(x) > c. The optimal classiÔ¨Åer h‚àóthen simpliÔ¨Åes to: h‚àó(x) ‚ààarg max y‚àà[L] X k‚àà[K] X y‚ààGk Œ≤k Œ±‚àó k ¬∑ Œ∑y(x). Since each class y belongs exactly one group, this is the same as: h‚àó(x) ‚ààarg max y‚àà[L] Œ≤[y] Œ±‚àó [y] ¬∑ Œ∑y(x), where [y] is the index of the group class y belongs to. Similarly, the optimal rejector r‚àósimpliÔ¨Åes to r‚àó(x) = 1 ‚áê‚áí min y‚àà[L] X k‚àà[K] X i‚ààGk  Œ≤k Œ±‚àó k ‚àí¬µ‚àó k  ¬∑ Œ∑i(x) ‚àí X k‚àà[K] X y‚ààGk Œ≤k Œ±‚àó k ¬∑ Œ∑y(x) > c, which is the same as: r‚àó(x) = 1 ‚áê‚áí max y‚àà[L] Œ≤[y] Œ±‚àó [y] ¬∑ Œ∑y(x) < X y‚Ä≤‚àà[L]  Œ≤[y‚Ä≤] Œ±‚àó [y‚Ä≤] ‚àí¬µ‚àó [y‚Ä≤]  ¬∑ Œ∑y‚Ä≤(x) ‚àíc, as desired. D.4 SPECIAL CASE OF BINARY GROUPS WITH BINARY LABELS In the special case where we have binary labels, each in a separate group, the rejector in Theorem 12 takes a much simpler form, with a constant threshold for each class. Corollary 13. Suppose Y = G = {0, 1}. Under Assumption 1, the optimal classiÔ¨Åer and rejector for Eq. (32) simpliÔ¨Åes to applying class-speciÔ¨Åc thresholds œÑ ‚àó 0 , œÑ ‚àó 1 ‚ààR and Œ≥‚àó‚àà(0, 1): h‚àó(x) = 1 (Œ∑1(x) > Œ≥‚àó) ; r‚àó(x) = 1 ‚áê‚áí Œ∑h‚àó(x)(x) < œÑ ‚àó h‚àó(x). Proof. When Y = G = {0, 1}, we have from Theorem 12 that the optimal classiÔ¨Åer takes the form: h‚àó(x) = arg max  Œ≤0 Œ±‚àó 0 ¬∑ Œ∑0(x), Œ≤1 Œ±‚àó 1 ¬∑ Œ∑1(x)  = 1  Œ≤0 Œ±‚àó 0 ¬∑ Œ∑0(x) < Œ≤1 Œ±‚àó 1 ¬∑ Œ∑1(x)  = 1 (Œ∑1(x) > Œ≥‚àó) , where Œ≥‚àó= Œ≤0/Œ±‚àó 0 Œ≤0/Œ±‚àó 0+Œ≤1/Œ±‚àó 1 , as desired; the last equality uses the fact that Œ∑0(x) + Œ∑1(x) = 1. Similarly, the optimal rejector is given by: r‚àó(x) = 1 ‚áê‚áí Œ≤h‚àó(x) Œ±‚àó h‚àó(x) ¬∑ Œ∑h‚àó(x)(x) <  Œ≤0 Œ±‚àó 0 ‚àí¬µ‚àó 0  ¬∑ Œ∑0(x) +  Œ≤1 Œ±‚àó 1 ‚àí¬µ‚àó 1  ¬∑ Œ∑1(x) ‚àíc. (36) 21
Published as a conference paper at ICLR 2024 Consider the case when h‚àó(x) = 1. We can then use the fact that Œ∑0(x) + Œ∑1(x) = 1 to re-write the right-hand side of Eq. (36) as: Œ≤1 Œ±‚àó 1 ¬∑ Œ∑1(x) <  Œ≤0 Œ±‚àó 0 ‚àí¬µ‚àó 0  ¬∑ (1 ‚àíŒ∑1(x)) +  Œ≤1 Œ±‚àó 1 ‚àí¬µ‚àó 1  ¬∑ Œ∑1(x) ‚àíc, which simpliÔ¨Åes to: Œ∑1(x) < œÑ ‚àó 1 , where œÑ ‚àó 1 = Œ≤0 Œ±‚àó 0 ‚àí¬µ‚àó 0 ‚àíc Œ≤0 Œ±‚àó 0 + ¬µ‚àó 1 ‚àí¬µ‚àó 0 . Similarly, when h‚àó(x) = 1, the right-hand side of Eq. (36) simpliÔ¨Åes to: Œ∑0(x) < œÑ ‚àó 0 , where œÑ ‚àó 0 = Œ≤1 Œ±‚àó 1 ‚àí¬µ‚àó 1 ‚àíc Œ≤1 Œ±‚àó 1 + ¬µ‚àó 0 ‚àí¬µ‚àó 1 , which completes the proof. D.5 BAYES-OPTIMAL CLASSIFIER AND REJECTOR FOR GENERALIZED L2R We next derive the Bayes-optimal classiÔ¨Åer and rejector for the generalized L2R risk in Eq. (14), which we restate below: Rrej gen(h, r) = œà (e1(h, r), . . . , e K(h, r)) + c ¬∑ P (r(x) = 1) , (37) where œà : [0, 1]K ‚ÜíR+ is a general function, and encompasses several common metrics (Lipton et al., 2014; Menon et al., 2013; Narasimhan et al., 2015a). One may equivalently write Eq. (37) in terms of the confusion matrix C(h, r): Rrej gen(h, r) = œà  P i‚ààG1 P jÃ∏=i Cij(h, r) P i‚ààG1 P j‚àà[L] Cij(h, r), . . . , P i‚ààGK P jÃ∏=i Cij(h, r) P i‚ààGK P j‚àà[L] Cij(h, r)  + c ¬∑  1 ‚àí X i,j Cij(h, r)  . Allowing for stochastic mechanisms F that are deÔ¨Åned by a distribution over classiÔ¨Åer-rejector pairs, we would like to minimize: min F œà  P i‚ààG1 P jÃ∏=i Cij(F) P i‚ààG1 P j‚àà[L] Cij(F), . . . , P i‚ààGK P jÃ∏=i Cij(F) P i‚ààGK P j‚àà[L] Cij(F)  + c ¬∑  1 ‚àí X i,j Cij(F)  , or equivalently: min F œà œÜ1(C(F)) œÄ1(C(F)), . . . , œÜK(C(F)) œÄK(C(F))  + c ¬∑  1 ‚àí X k œÄk(C(F))  , (38) where œÜk(C) = P i‚ààGk P jÃ∏=i Cij(F) and œÄk(C) = P i‚ààGk P j‚àà[L] Cij(F). Theorem 7 (restated). For any non-linear œà, there exists coefÔ¨Åcients u(i), v(i) ‚ààRK, i ‚àà[K + 1] and a parameter ŒΩ ‚àà‚àÜK+1, such that, the generalized L2R in Eq. (14) is minimized by a stochastic mechanism that for any x, predicts using (h(i), r(i)) with probability ŒΩi: h(i)(x) ‚ààarg max y u(i) y ¬∑ Œ∑y(x), i ‚àà[K + 1]; r(i)(x) = 1 ‚áê‚áí max y u(i) y ¬∑ Œ∑y(x) < X y‚Ä≤ v(i) y‚Ä≤ ¬∑ Œ∑y‚Ä≤(x) ‚àíc, i ‚àà[K + 1]. 22
Published as a conference paper at ICLR 2024 Proof. One may reformulate Eq. (38) as an optimization over the space of all confusion statistics realizable through distribution over classiÔ¨Åers-rejector pairs C, or equivalently over a smaller space of transformed confusion matrices: C‚Ä≤ = {(œÜ, œÄ), where œÜ = (œÜ1(C), . . . , œÜK(C)), and œÄ = (œÄ1(C), . . . , œÄK(C)) | ‚àÄC ‚ààC}. We can then solve Eq. (38) by solving: min (œÜ,œÄ)‚ààC‚Ä≤ œà œÜ1 œÄ1 , . . . , œÜK œÄK  + c ¬∑  1 ‚àí X k œÄk  . (39) We know from Lemma 9 that C is convex; consequently, it is easy to see that C‚Ä≤ is also convex. Therefore any point (œÜ, œÄ) ‚ààC‚Ä≤ can be expressed as a convex combination of extreme points of C‚Ä≤. In fact, following Proposition 10 in Narasimhan et al. (2022b), any (œÜ, œÄ) ‚ààC‚Ä≤ can be expressed as convex combination of K + 1 extreme points (œÜ(1), œÄ(1)), . . . (œÜ(K+1), œÄ(K+1)), each of which is a unique linear minimizer over C‚Ä≤: (œÜ(i), œÄ(i)) ‚ààarg min (œÜ,œÄ)‚ààC‚Ä≤ X k w(i) k ¬∑ œÜk + X k q(i) k ¬∑ œÄk for coefÔ¨Åcients w(i), q(i) ‚ààRK, i ‚àà[K + 1]. Therefore this is also true for the minimizer (œÜ‚àó, œÄ‚àó) ‚ààC‚Ä≤ of Eq. (39). All that remains to show is that these K +1 linear minimizers are realized by K +1 classiÔ¨Åer-rejector pairs of the form in the theorem statement. To show this, we note that for coefÔ¨Åcients w(i), q(i) ‚ààRK, minimizing the linear objective over C‚Ä≤: min (œÜ,œÄ)‚ààC‚Ä≤ X k w(i) k ¬∑ œÜk + X k q(i) k ¬∑ œÄk is equivalent to minimizing the same objective over the boundary points of C‚Ä≤, and equivalently, over deterministic classiÔ¨Åer-rejector pairs: min h,r X k w(i) k ¬∑ œÜk(C(h, r)) + X k q(i) k ¬∑ œÄk(C(h, r)). (40) Expanding the above objective, we get: X k w(i) k ¬∑ E [1(y Ã∏= h(x), r(x) = 0, y ‚ààGk)] + X k q(i) k ¬∑ E [1(r(x) = 0, y ‚ààGk)] , which is the same as: Ex Ô£Æ Ô£∞X k w(i) k X y‚ààGk Œ∑y(x) ¬∑ 1(y Ã∏= h(x), r(x) = 0) + X k q(i) k X y‚ààGk Œ∑y(x) ¬∑ 1(r(x) = 0) Ô£π Ô£ª, or: Ex  X y w(i) [y] ¬∑ Œ∑y(x) ‚àíw(i) [h(x)] ¬∑ Œ∑h(x)(x) + X y q(i) [y] ¬∑ Œ∑y(x)  ¬∑ 1(r(x) = 0)  . The minimizer of Eq. (40) then takes the form: h(i)(x) ‚ààarg max y‚àà[L] w(i) [y] ¬∑ Œ∑y(x); r(i)(x) = 1 ‚áê‚áí max y‚àà[L] w(i) [y] ¬∑ Œ∑y(x) < X y‚àà[L] (w(i) [y] + q(i) [y]) ¬∑ Œ∑y(x). Setting u(i) y = w(i) [y] and v(i) y = w(i) [y] + q(i) [y] + c, the above is equivalent to: h(i)(x) ‚ààarg max y‚àà[L] u(i) y ¬∑ Œ∑y(x); r(i)(x) = 1 ‚áê‚áí max y‚àà[L] u(i) y ¬∑ Œ∑y(x) < X y‚àà[L] v(i) y ¬∑ Œ∑y(x) ‚àíc. It thus follows that the minimizer (œÜ‚àó, œÄ‚àó) = (œÜ(C(F ‚àó)), œÄ(C(F ‚àó)) to Eq. (39) is real- ized by a stochastic mechanism F ‚àóthat randomizes over the K + 1 classiÔ¨Åer-rejector pairs (h(1), r(1)), . . . , (h(K+1), r(K+1)). 23
Published as a conference paper at ICLR 2024 Algorithm 3 Lagrangian-based Plug-in for Balanced Error 1: Input: Rejection cost c, Pre-trained p : X ‚Üí‚àÜL, Sample S 2: Parameters: Iterations M, T, Step-sizes Œæ¬µ, ŒæŒ±, Initial Œ±(0) ‚àà(0, K)K, Lower bound Œ∫ > 0 3: For m = 0 to M 4: h(m)(x) = arg maxy‚àà[L] u(m) y ¬∑ py(x), where u(m) y = 1 Œ±(m) [y] 5: Initialize ¬µ(m,0) ‚ààRK 6: For t = 0 to T ‚àí1 7: ¬µ(m,t+1) k = ¬µ(m,t) k + Œæ¬µ ¬∑  Œ±(m) i ‚àíK |S| P (x,y)‚ààS 1 (r(x) = 0, y ‚ààGk)  , ‚àÄk ‚àà[K] 8: r(m,t+1)(x) = 1 ‚áê‚áí maxy‚àà[L] u(m) y ¬∑ py(x) < P y‚Ä≤‚àà[L] v(m) y‚Ä≤ ¬∑ py‚Ä≤(x) ‚àíc, 9: where v(m) y = 1 Œ±(m) [y] ‚àí¬µ(m,t+1) [y] . 10: End For 11: Œ±(m+1) k = Œ±(m) k ‚àíŒæŒ± ¬∑  ¬µ(m,T ) k ‚àí 1 (Œ±(m) k )2 ¬∑ ÀÜek(h(m), r(m,T ))  12: Œ±(m+1) k = proj[Œ∫, K‚àíŒ∫]  Œ±(m) k  , where proj[a,b](z) = max{min{z, b}, a} 13: End For 14: Return: (h(M), r(M,T )) E PLUG-IN APPROACH: FURTHER DETAILS We provide further details about the plug-in approach in ¬ß4. E.1 RE-PARAMETERIZING THE REJECTOR FOR THE BALANCED ERROR One can further prune the search space over multipliers ÀÜ¬µ by re-parameterizing the rejection criterion in Eq. (13) to search over only K ‚àí1 parameters. To this end, using the fact that P y‚àà[L] py(x) = 1, we re-write the criterion as: max y‚àà[L] 1 ÀÜŒ±[y] ¬∑ py(x) < X y‚Ä≤‚àà[L] 1 ÀÜŒ±[y‚Ä≤] ¬∑ py‚Ä≤(x) ‚àí X y‚Ä≤‚àà[L] (ÀÜ¬µ[y‚Ä≤] ‚àíÀÜ¬µK) ¬∑ py‚Ä≤(x) ‚àíÀÜ¬µK X y‚Ä≤‚àà[L] py‚Ä≤(x) ‚àíc, which is the same as: max y‚àà[L] 1 ÀÜŒ±[y] ¬∑ py(x) < X y‚Ä≤‚àà[L] 1 ÀÜŒ±[y‚Ä≤] ¬∑ py‚Ä≤(x) ‚àí X k‚àà[K‚àí1] (ÀÜ¬µk ‚àíÀÜ¬µK) ¬∑ X y‚Ä≤‚ààGk py‚Ä≤(x) ‚àíÀÜ¬µK ‚àíc. This can be equivalently re-paramterized as: max y‚àà[L] 1 ÀÜŒ±[y] ¬∑ py(x) < X y‚Ä≤‚àà[L] 1 ÀÜŒ±[y‚Ä≤] ¬∑ py‚Ä≤(x) ‚àí X k‚àà[K‚àí1] X y‚Ä≤‚ààGk ÀÜŒªk ¬∑ py‚Ä≤(x) ‚àíÀÜœÑ, where ÀÜŒªk = ÀÜ¬µk ‚àíÀÜ¬µK, for k ‚àà[K ‚àí1], and ÀÜœÑ = ÀÜ¬µK ‚àíc. Note that in practice, one is often prescribed a target rejection rate, and picks the rejection cost c to achieve this rejection rate. For Ô¨Åxed ÀÜŒª1, . . . , ÀÜŒªK‚àí1, one may equivalently pick the threshold ÀÜœÑ (instead of cost c) to reach the desired rejection rate. So given a target rejection rate, all one needs to do is to apply a brute-force search to tune the remaining K ‚àí1 unknowns to minimize the balanced error on a validation set, with the threshold ÀÜœÑ set to a percentile that matches the desired rejection rate. When K = 2 (head and tail), i.e., G = {0, 1}, this would mean that we only have a single parameter ÀÜŒª0 = ÀÜ¬µ0 ‚àíÀÜ¬µ1 to tune, which can be efÔ¨Åciently done using a simple line search. E.2 LAGRANGIAN-BASED PLUG-IN APPROACH As an alternative to plug-in approach in Algorithm 1, we explore a Lagrangian-based approach that handles the constraints more intricately. We begin by introducing multipliers ¬µ ‚ààRK for the K 24
Published as a conference paper at ICLR 2024 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 Standard Error Chow CSS Chow [BCE] Plug-in [Balanced] (a) CIFAR-100 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.1 0.2 0.3 0.4 Standard Error Chow CSS Chow [BCE] Plug-in [Balanced] (b) Image Net 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Standard Error Chow CSS Chow [BCE] Plug-in [Balanced] (c) i Naturalist Figure 4: Balanced L2R: Standard 0-1 error as a function of proportion of rejections. 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.1 0.2 0.3 0.4 0.5 Standard Error Chow CSS Chow [DRO] Plug-in [Worst] (a) CIFAR-100 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.1 0.2 0.3 0.4 Standard Error Chow CSS Chow [DRO] Plug-in [Worst] (b) Image Net 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Standard Error Chow CSS Chow [DRO] Plug-in [Worst] (c) i Naturalist Figure 5: Worst-group L2R: Standard 0-1 error as a function of proportion of rejections. constraints in the optimization problem in Eq. (7) and writing out the Lagrangian: L(h, r, Œ±; ¬µ) = X k‚àà[K] 1 Œ±k ¬∑ P (y Ã∏= h(x), r(x) = 0, y ‚ààGk) + c ¬∑ P (r(x) = 1) ‚àí X k‚àà[K] ¬µk ¬∑ (K ¬∑ P (r(x) = 0, y ‚ààGk) ‚àíŒ±k) , and formulate an equivalent saddle point optimization problem: min Œ±,h,r max ¬µ‚ààRK L(h, r, Œ±; ¬µ). (41) For a Ô¨Åxed Œ±, we can solve the min-max problem over h, r and ¬µ by alternating between gradient- ascent updates on the multipliers ¬µ: ¬µ(t+1) k = ¬µ(t) k + Œæ¬µ ¬∑ (Œ±k ‚àíK ¬∑ P (r(x) = 0, y ‚ààGk)) , ‚àÄk ‚àà[K], for step size Œæ¬µ > 0, and a full minimization over h and r. The procedure outlined in Algorithm 3 then solves the saddle point optimization problem in Eq. (41) by performing gradient-descent on Œ±, and the combination of the gradient-ascent and full minimization procedure described above over h, r and ¬µ. It is worth noting that this Lagrangian procedure is made possible by viewing Eq. (7) as an opti- mization problem over the space of the space of realizable confusion statistics C (Narasimhan, 2018; Narasimhan et al., 2022b), which we can then show is a continuous constrained optimization problem that is amenable be solved with Lagrangian style updates. 25
Published as a conference paper at ICLR 2024 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.2 0.4 0.6 0.8 1.0 Tail Error - Head Error Chow CSS Chow [BCE] Plug-in [Balanced] (a) CIFAR-100 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Tail Error - Head Error Chow CSS Chow [BCE] Plug-in [Balanced] (b) Image Net 0.00 0.25 0.50 0.75 Proportion of Rejections ‚àí0.05 0.00 0.05 0.10 Tail Error - Head Error Chow CSS Chow [BCE] Plug-in [Balanced] (c) i Naturalist Figure 6: Balanced L2R: Difference between tail and head errors as a function of proportion of rejections. 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.2 0.4 0.6 0.8 1.0 Tail Error - Head Error Chow CSS Chow [DRO] Plug-in [Worst] (a) CIFAR-100 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Tail Error - Head Error Chow CSS Chow [DRO] Plug-in [Worst] (b) Image Net 0.00 0.25 0.50 0.75 Proportion of Rejections ‚àí0.025 0.000 0.025 0.050 0.075 0.100 0.125 Tail Error - Head Error Chow CSS Chow [DRO] Plug-in [Worst] (c) i Naturalist Figure 7: Worst-group L2R: Difference between tail and head errors as a function of proportion of rejections. F ADDITIONAL EXPERIMENTAL RESULTS We discuss the data splits, hyper-parameter choices and further implementation details in our exper- iments. We also include additional experimental plots. All results are averaged over 5 trials. We report 95% conÔ¨Ådence intervals in Table 2. F.1 HYPER-PARAMETER CHOICES We apply the same data pre-processing as Menon et al. (2021a). We trained all the models using SGD with a momentum of 0.9 and a weight decay of 10‚àí4. We summarise the hyper-parameter choices in Table 3 below. Dataset #classes ntrain ntest Tail prop. Model Base LR Schedule Epochs Batch size CIFAR-100-LT 100 50,000 10,000 0.03 CIFAR Res Net-32 0.4 anneal 256 128 Image Net-LT 1000 1,281,167 50,000 0.02 Res Net-50 0.4 cosine 200 512 i Naturalist 2018 8143 437,513 24,426 0.12 Res Net-50 0.4 cosine 200 1024 Table 3: Dataset details and hyper-parameter choices. For CIFAR-100, we apply a warm up with a linear learning rate for 15 steps until we reach the base learning rate. We apply a learning rate decay of 0.1 at the 96th, 192nd and 224th epochs. For Image Net and i Naturalist, we apply a warm up with a linear learning rate for 5 steps until we reach the base learning rate. We apply a learning rate decay of 0.1 at the 45th, 100th and 150th epochs. 26
Published as a conference paper at ICLR 2024 F.2 DATASET SPLITS The train, test and validations samples all follow the same long-tailed label distributions. Each dataset comes with an original test set with equal proportions of all labels. We re-weight the samples in the original test set to replicate the same label proportions as the training set. Furthermore, we hold out 20% of the original test set as a validation sample and use the remaining as the test sample. F.3 IMPLEMENTATION DETAILS For the proposed plug-in method for the balanced error (Algorithm 1), we set the number of iterations M to 10, and tune the single parameter ÀÜŒª0 = ÀÜ¬µ0 ‚àíÀÜ¬µ1 (see Appendix E.1) from three values {1, 6, 11}. For the proposed plug-in method for the worst-group error (Algorithm 2), we set the number of outer iterations T to 25 and the step-size Œæ to 1; for the inner call to Algorithm 1, we set M to 10 and tune the parameter ÀÜŒª0 from {1, 6, 11}. We implement the cost-sensitive softmax (CSS) loss of Mozannar & Sontag (2020) described in Equation 3 in their paper. We set the cost vector c(1), . . . , c(L + 1) in this loss to c(i) = 1(y Ã∏= i), ‚àÄi ‚àà[L] and c(L + 1) to the rejection cost c. For Chow [Balanced], there exists a plethora of options in the long-tail literature to train the base model (see ¬ß2.2 for a brief list of representative approaches). We pick the logit-adjusted cross-entropy loss of Menon et al. (2021a) as a representative approach. For Chow [DRO], as prescribed in Jones et al. (2021), we use the group DRO approach proposed in Sagawa et al. (2020), but modify the inner weighted minimization step to use a logit-adjusted loss (Narasimhan & Menon, 2021; Wang et al., 2023). This modiÔ¨Åcation was necessary to adapt the approach of Sagawa et al. (2020) to multi-class classiÔ¨Åcation with a large number of classes. Their method requires tuning a step-size for DRO and a regularization parameter C. We pick these parameters from the sets {0.0005, 0.001} and {1.0, 2.0} respectively, by maximizing the worst-group error on the validation sample. F.4 TRADE-OFF PLOTS: FURTHER DETAILS To generate the trade-off plots in Figure 3‚Äì7 for each method, we train rejectors with different rejection rates. For this, one may vary the cost parameter c, or equivalently, vary the rejection threshold in the case of Chow‚Äôs rule and the threshold ÀÜœÑ in the proposed plug-in method (see Appendix E.1). For a target rejection rate, this can be done with a simple percentile computation. For the CSS baseline, we re-train a model for different values of c in {0.0, 0.1, 0.5, 0.75, 0.85, 0.91, 0.95, 0.97, 0.99}. Unfortunately, we Ô¨Ånd that with the Image Net and i Naturalist datasets, the resulting models do not exceed a rejection rate of 40% and 75% respectively, despite us using the entire range of c ‚àà[0, 1]. F.5 ADDITIONAL PLOTS We present additional plots of the standard 0-1 error as a function of rejection rate for different methods in Figures 4‚Äì5. We also present plots of the difference between the tail and head errors as a function of rejection rates in Figures 6‚Äì7. F.6 COMPARISON TO EXHAUSTIVE SEARCH OVER Œ± We repeated the CIFAR-100 experiments in ¬ß6 choosing the multiplier parameter from a Ô¨Åner grid of values: {0.25, 0.5, 0.75, . . . , 11}. We additionally replaced our heuristic procedure in ¬ß4.2 with an exhaustive grid search over group prior Œ±, choosing the head-to-tail ratio from the range {0, 0.01, 0.02, . . . , 1.0}. We pick the parameter combination that yields the lowest balanced error on the held-out validation sample. We show below the balanced error on the test and validation samples. Although the grid search provides gains on the validation set, on the test set, our heuristic approach to picking the multipliers 27
Published as a conference paper at ICLR 2024 and group priors yields metrics that are only slightly worse than grid-search. Moreover, the differences are within standard errors. Test Validation Chow 0.509 ¬± 0.002 0.498 ¬± 0.011 Plug-in [Balanced] 0.291 ¬± 0.008 0.282 ¬± 0.007 Plug-in [Balanced, grid search] 0.284 ¬± 0.007 0.264 ¬± 0.008 28
