# References

Standard (1) arg maxy ηy(x) maxy ηy(x) < 1 −c Chow (1970) Conditional (22) arg maxy ηy(x) maxy ηy(x) < 1 −c′ Lemma 8 (App. C) Balanced (6) arg maxy uy · ηy(x) maxy uy · ηy(x) < P j vj · ηj(x) −c Theorem 1 Generalized (14) Stochastic combination of (h, r) pairs of the form in row 3 Theorem 3 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Balanced Error Chow's rule Plug-in [Balanced] Plug-in [Worst] (a) 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.1 0.2 0.3 0.4 Conditional Head Error Chow's rule Plug-in [Balanced] Plug-in [Worst] (b) 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.2 0.4 0.6 0.8 Conditional Tail Error Chow's rule Plug-in [Balanced] Plug-in [Worst] (c) 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Tail Error - Head Error Chow's rule Plug-in [Balanced] Plug-in [Worst] (d) Figure 1: Comparison of balanced error as a function of proportion of samples rejected on the Image Net-LT dataset. We compare Chow’s rule with the proposed plug-in methods. We also include plots of the individual head and tail errors, and the differences between them. We designate classes with 20 or fewer samples in the training set as “tail” classes, and the remaining as “head” classes. The base classiﬁer uses a Res Net-50 architecture. Chow’s rule is seen to be substantially better on the head group compared to the tail group. In contrast, the proposed plug-in rules yield signiﬁcantly lower balanced (worst-group) error. They are also seen to yield a lower gap between the tail and head errors. (i) We ﬁrst derive the Bayes-optimal classiﬁer and rejector for the balanced error metric, and show that they have a fundamentally different form from classical baselines such as Chow’s rule (Chow, 1970), which reject samples with low prediction conﬁdence (§3, Table 1, Theorem 1). (ii) We show that vanilla modiﬁcations to Chow’s rule such as changing the loss used in the base model training are also theoretically sub-optimal for the balanced error. As an alternative, we propose a novel plug-in approach that seeks to mimic the Bayes-optimal solution without making any changes to the base model training (§4). (iii) We then extend our Bayes-optimal characterization to any evaluation metric that can be expressed as a general function of the per-class errors (Theorem 3), and as a concrete example, showcase how to extend our plug-in approach to minimize the worst-group error (§5). (iv) Through experiments on benchmark long-tailed image classiﬁcation datasets, we demonstrate that our plug-in approach yields signiﬁcantly better trade-offs than Chow’s rule, and is competitive with variants of Chow that require changes to the training process (§6). 2 LEARNING TO REJECT AND LONG-TAIL CLASSIFICATION We begin with some background material. Let X be an instance space, Y = [L] .= {1, 2, . . . , L} be the label space, and P be the underlying distribution over X × Y. Furthermore, let ηy(x) = P(y | x) denote the conditional distribution over labels. Given a training set S = {(xn, yn)}n∈[N] comprising N i.i.d. draws from P, multi-class classiﬁcation canonically seeks a classiﬁer h: X →Y with low misclassiﬁcation error R0-1(h) = P(y ̸= h(x)). We may parameterise h(x) = arg maxy′∈Y fy′(x) for a scorer f : X →RL, which is typically trained by minimizing an empirical surrogate risk on S. 2.1 LEARNING TO REJECT We are interested in settings where the classiﬁer is provided the option of abstaining on select samples. To this end, we consider the goal of learning a classiﬁer h: X →Y and a rejector r: X →{0, 1}. For a given instance x, we abstain on a sample whenever r(x) = 1, and predict h(x) otherwise. 2
Published as a conference paper at ICLR 2024 Prior literature has explored minimizing the standard misclassiﬁcation error with a constant penalty for abstaining on a sample (Cortes et al., 2016): Rrej 0-1(h, r) = P (y ̸= h(x), r(x) = 0) + c · P (r(x) = 1) . (1) Intuitively, when r chooses not to abstain, we incur the usual misclassiﬁcation error; otherwise, we incur the cost c > 0. Under mild distributional assumptions, this objective can be equivalently seen as constraining the proportion of abstentions to be within a budget, a formulation often referred to as selective classiﬁcation (Geifman & El-Yaniv, 2017; Gangrade et al., 2021). More precisely, via a Lagrangian analysis (following, e.g., the Neyman-Pearson lemma (Neyman & Pearson, 1933)), for any such abstention budget, there exists an equivalent cost c in Eq. (1); see
