# Appendix e

F for details). We train a Res Net-32 (50) model for CIFAR (Image Net and i Naturalist). 8
Published as a conference paper at ICLR 2024 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.1 0.2 0.3 0.4 0.5 0.6 Balanced Error Chow CSS Chow [BCE] Plug-in [Balanced] 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Balanced Error Chow CSS Chow [BCE] Plug-in [Balanced] 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 Balanced Error Chow CSS Chow [BCE] Plug-in [Balanced] 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.2 0.4 0.6 0.8 1.0 Worst-group Error Chow CSS Chow [DRO] Plug-in [Worst] (a) CIFAR-100 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.2 0.4 0.6 0.8 Worst-group Error Chow CSS Chow [DRO] Plug-in [Worst] (b) Image Net 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.1 0.2 0.3 0.4 Worst-group Error Chow CSS Chow [DRO] Plug-in [Worst] (c) i Naturalist Figure 3: Balanced and worst-group errors as functions of proportion of rejections. Lower is better. Evaluation metrics. For each dataset, we divide the classes into two groups, namely head and tail. We designate all classes with 20 or fewer samples in the training set as tail classes (Liu et al., 2019), and the remaining as the head. We evaluate both the balanced error and the worst-group error across the head and tail groups. We train classiﬁers and rejectors with different rejection costs c, and plot these metrics as a function of the proportion of rejections. We summarize the performance across different rejection rates by computing the areas under the respective curves (averaged over 5 trials); this summary metric is referred to as the area under risk-coverage curve (AURC) (Moon et al., 2020). Comparisons. We compare against two representative L2R baselines that optimize the standard 0-1 error: (i) Chow’s rule, which trains a base model by optimizing the CE loss, and thresholds the estimated max probability (Eq. (2)); and (ii) the cost-sensitive softmax (CSS) loss of Mozannar & Sontag (2020), which jointly trains a classiﬁer and rejector using a surrogate loss (with the Res Net model predicting L + 1 logits, and the (L + 1)-th denoting the ‘abstain’ option). We evaluate our plug-in methods for the balanced error (Alg. 1) and the worst-case error (Alg. 2), both of which use the same base model as Chow’s rule. We also evaluate variants of Chow’s rule (§4.1), where the base model is trained with either a balanced cross-entropy (BCE) loss implemented through logit adjustments (Menon et al., 2021a), or the DRO loss prescribed by Jones et al. (2021). Results. We present plots of the balanced and worst-group errors as a function of the rejection rate (for varying values of c) in Figure 3. We summarize the area under these curves in Table 2. On all three datasets, the proposed plug-in methods are substantially better than both Chow’s rule and the CSS baseline. The plug-in methods are also comparable to or better than the variants of Chow’s rule that uses a balanced or DRO base loss. As shown in Figures 4–5 i
