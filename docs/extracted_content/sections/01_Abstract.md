# Abstract

Learning to reject (L2R) is a classical problem where one seeks a classiﬁer capable of abstaining on low-conﬁdence samples. Most prior work on L2R has focused on minimizing the standard misclassiﬁcation error. However, in many real-world applications, the label distribution is highly imbalanced, necessitating alternate evaluation metrics such as the balanced error or the worst-group error that enforce equitable performance across both the head and tail classes. In this paper, we establish that traditional L2R methods can be grossly sub-optimal for such metrics, and show that this is due to an intricate dependence in the objective between the label costs and the rejector. We then derive the form of the Bayes-optimal classiﬁer and rejector for the balanced error, propose a novel plug-in approach to mimic this solution, and extend our results to general evaluation metrics. Through experiments on benchmark image classiﬁcation tasks, we show that our approach yields better trade-offs in both the balanced and worst-group error compared to L2R baselines.
