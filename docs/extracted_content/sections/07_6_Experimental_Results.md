# 6. Experimental Results

We present experiments on long-tailed image classiﬁcation tasks to showcase that proposed plug-in approaches for the balanced error (§4) and the worst-group error (§5) yield signiﬁcantly better trade-offs than Chow’s rule, despite using the same base model, and are competitive with variants of Chow’s rule which require re-training the base model with a modiﬁed loss. Datasets. We replicate the long-tail experimental setup from Menon et al. (2021a). We use long-tailed versions of CIFAR-100 (Krizhevsky, 2009), Image Net (Deng et al., 2009) and i Naturalist (Van Horn et al., 2018). For CIFAR-100, we downsample the examples per label following the Exp proﬁle of Cao et al. (2019), and for Image Net, we use the long-tail version provided by Liu et al. (2019). The train, test and validation splits have the same label distributions (see Appendix F for details). We train a Res Net-32 (50) model for CIFAR (Image Net and i Naturalist). 8
Published as a conference paper at ICLR 2024 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.1 0.2 0.3 0.4 0.5 0.6 Balanced Error Chow CSS Chow [BCE] Plug-in [Balanced] 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Balanced Error Chow CSS Chow [BCE] Plug-in [Balanced] 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 Balanced Error Chow CSS Chow [BCE] Plug-in [Balanced] 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.2 0.4 0.6 0.8 1.0 Worst-group Error Chow CSS Chow [DRO] Plug-in [Worst] (a) CIFAR-100 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.2 0.4 0.6 0.8 Worst-group Error Chow CSS Chow [DRO] Plug-in [Worst] (b) Image Net 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.1 0.2 0.3 0.4 Worst-group Error Chow CSS Chow [DRO] Plug-in [Worst] (c) i Naturalist Figure 3: Balanced and worst-group errors as functions of proportion of rejections. Lower is better. Evaluation metrics. For each dataset, we divide the classes into two groups, namely head and tail. We designate all classes with 20 or fewer samples in the training set as tail classes (Liu et al., 2019), and the remaining as the head. We evaluate both the balanced error and the worst-group error across the head and tail groups. We train classiﬁers and rejectors with different rejection costs c, and plot these metrics as a function of the proportion of rejections. We summarize the performance across different rejection rates by computing the areas under the respective curves (averaged over 5 trials); this summary metric is referred to as the area under risk-coverage curve (AURC) (Moon et al., 2020). Comparisons. We compare against two representative L2R baselines that optimize the standard 0-1 error: (i) Chow’s rule, which trains a base model by optimizing the CE loss, and thresholds the estimated max probability (Eq. (2)); and (ii) the cost-sensitive softmax (CSS) loss of Mozannar & Sontag (2020), which jointly trains a classiﬁer and rejector using a surrogate loss (with the Res Net model predicting L + 1 logits, and the (L + 1)-th denoting the ‘abstain’ option). We evaluate our plug-in methods for the balanced error (Alg. 1) and the worst-case error (Alg. 2), both of which use the same base model as Chow’s rule. We also evaluate variants of Chow’s rule (§4.1), where the base model is trained with either a balanced cross-entropy (BCE) loss implemented through logit adjustments (Menon et al., 2021a), or the DRO loss prescribed by Jones et al. (2021). Results. We present plots of the balanced and worst-group errors as a function of the rejection rate (for varying values of c) in Figure 3. We summarize the area under these curves in Table 2. On all three datasets, the proposed plug-in methods are substantially better than both Chow’s rule and the CSS baseline. The plug-in methods are also comparable to or better than the variants of Chow’s rule that uses a balanced or DRO base loss. As shown in Figures 4–5 in Appendix F.5, Chow’s rule is unsurprisingly the best on the standard 0-1 error. On the other hand, Figures 6–7 show that our plug-in methods yield a lower gap between the tail and head errors compared to the L2R baselines. Finally, in Appendix F.6, we show on CIFAR-100 that the heuristic we employ in our plug-in approach to pick the group coverage α (§4.2) performs comparable to an exhaustive grid search over α.
