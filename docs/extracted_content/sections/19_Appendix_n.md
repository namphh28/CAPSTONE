# Appendix n

D.4). Remark 1 (Sub-optimality of L2R Baselines). Unlike the classical Chow’s rule in Eq. (2) or its conditional variant in Eq. (23), which apply a constant threshold to the maximum conditional probability maxy ηy(x), the optimal rejector in Theorem 1 takes a fundamentally different form, involving all class-probabilities. In fact, it can be seen as applying a sample-dependent threshold to re-weighted probabilities. This suggests that Chow’s rule is sub-optimal for the balanced L2R risk. This also suggests that approaches which optimize a surrogate loss for the L2R risk (Cortes et al., 2016; Geifman & El-Yaniv, 2017; Ramaswamy et al., 2018; Ni et al., 2019; Mozannar & Sontag, 2020; Charoenphakdee et al., 2021) are also sub-optimal for the balanced L2R risk, as they are also designed to learn the same thresholded rejector in Eq. (2) that Chow’s rule seeks to mimic. 4 ALGORITHMS FOR BALANCED L2R We next present an algorithm for learning a classiﬁer and rejector that is optimal for the balanced L2R risk in Eq. (6). We begin by exploring simple modiﬁcations to Chow’s rule. 4.1 CHOW’S RULE WITH MODIFIED BASE LOSS One obvious variation to Chow’s rule is to modify the base model training to optimize a balanced version of the CE loss in Eq. (3): f bal = argminf X k∈[K] 1 πk · E [1(y ∈Gk) · ℓce(y, f(x))] . (9) One may then implement Chow’s rule via the class probability estimates pbal y (x) ∝exp(f bal y (x)): hbal(x) ∈arg maxy∈[L] pbal y (x); rbal(x) = 1 ⇐⇒maxy∈[L] pbal y (x) < 1 −c. (10) 5
Published as a conference paper at ICLR 2024 Although intuitive, this simple modiﬁcation to Chow’s rule does not yield the optimal classiﬁer and rejector for the balanced L2R risk. Lemma 2. The variant of Chow’s rule in Eq. (10), where the base model is trained with the balanced cross-entropy loss, results in a classiﬁer and rejector of the form: hbal(x) = arg max y∈[L] 1 π[y] · ηy(x) rbal(x) = 1 ⇐⇒ max y∈[L] 1 π[y] · ηy(x) <  X y′∈[L] 1 π[y′] · ηy′(x)  · (1 −c), (11) where [y] is the index of the group to which class y belongs. There are two key differences between the above rejector and the optimal rejector in Theorem 1. First, in the above rejector, the abstention cost c is part of a multiplicative term whereas it is part of an additive term in the rejector in Theorem 1. Second, the per-class weights in the above rejector depend on the ﬁxed group priors πk, whereas those in Theorem 1 depend more intricately on the underlying distribution through the parameters α∗ k (which depend on the optimal rejector) and multipliers µ∗ k. More generally, any re-weighting of the base loss with constant coefﬁcients will result in a rejector with a multiplicative penalty term different from the form in Theorem 1 (se
