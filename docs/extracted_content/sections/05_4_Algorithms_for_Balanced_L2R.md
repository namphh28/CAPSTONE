# 4. Algorithms for Balanced L2R

FOR BALANCED L2R We next present an algorithm for learning a classiﬁer and rejector that is optimal for the balanced L2R risk in Eq. (6). We begin by exploring simple modiﬁcations to Chow’s rule. 4.1 CHOW’S RULE WITH MODIFIED BASE LOSS One obvious variation to Chow’s rule is to modify the base model training to optimize a balanced version of the CE loss in Eq. (3): f bal = argminf X k∈[K] 1 πk · E [1(y ∈Gk) · ℓce(y, f(x))] . (9) One may then implement Chow’s rule via the class probability estimates pbal y (x) ∝exp(f bal y (x)): hbal(x) ∈arg maxy∈[L] pbal y (x); rbal(x) = 1 ⇐⇒maxy∈[L] pbal y (x) < 1 −c. (10) 5
Published as a conference paper at ICLR 2024 Although intuitive, this simple modiﬁcation to Chow’s rule does not yield the optimal classiﬁer and rejector for the balanced L2R risk. Lemma 2. The variant of Chow’s rule in Eq. (10), where the base model is trained with the balanced cross-entropy loss, results in a classiﬁer and rejector of the form: hbal(x) = arg max y∈[L] 1 π[y] · ηy(x) rbal(x) = 1 ⇐⇒ max y∈[L] 1 π[y] · ηy(x) <  X y′∈[L] 1 π[y′] · ηy′(x)  · (1 −c), (11) where [y] is the index of the group to which class y belongs. There are two key differences between the above rejector and the optimal rejector in Theorem 1. First, in the above rejector, the abstention cost c is part of a multiplicative term whereas it is part of an additive term in the rejector in Theorem 1. Second, the per-class weights in the above rejector depend on the ﬁxed group priors πk, whereas those in Theorem 1 depend more intricately on the underlying distribution through the parameters α∗ k (which depend on the optimal rejector) and multipliers µ∗ k. More generally, any re-weighting of the base loss with constant coefﬁcients will result in a rejector with a multiplicative penalty term different from the form in Theorem 1 (see Appendix A.2). 4.2 PLUG-IN APPROACH WITH NO CHANGES TO THE BASE LOSS As an alternative to modifying the base loss in Chow’s rule, we propose a new plug-in approach that seeks to directly mimic the optimal classiﬁer and rejector for the balanced L2R risk. Like standard Chow’s rule, we assume access to only a class probability estimator py(x) trained to minimize the softmax CE loss in Eq. (3). We then wish to approximate the Bayes-optimal classiﬁer and rejector from Theorem 1 by plugging in estimates py(x) of the class probabilities: ˆh(x) = arg max y∈[L] 1 ˆα[y] · py(x); (12) ˆr(x) = 1 ⇐⇒ max y∈[L] 1 ˆα[y] · py(x) < X y′∈[L]  1 ˆα[y′] −ˆµ[y′]  · py′(x) −c, (13) for choices of ˆα ∈(0, K)K and ˆµ ∈RK. One may carry out an exhaustive search to pick ˆα and ˆµ so that the resulting classiﬁer-rejector pair yields the lowest balanced error on a held-out sample. However, searching over 2K parameters can quickly become infeasible even when K is small. To make the search more efﬁcient, we exploit the additional constraint in Theorem 1 that requires ˆαk to match the rejector’s coverage for group Gk, i.e., ˆαk = K · P(y ∈Gk, ˆr(x) = 0). Speciﬁcally, we propose applying a grid search over only the multipliers ˆµ, while using a power-iteration-like heuristic to choose ˆα. Starting with initial values α(0) ∈(0, K)K, we propose alternating between constructing the classiﬁer-rejector pair and updating the group priors using the rejector’s coverage: • Construct (h(m), r(m)) using equations 12–13 with ˆα = α(m) and the ﬁxed multipliers ˆµ; • Set α(m+1) k = K · P(y ∈Gk, r(m)(x) = 0). One may repeat these steps until the iterates α(t)s converge to a stationary point, and return the corresponding classiﬁer and rejector. For moderate number of groups K, one may apply the above procedure for different values of multipliers ˆµ ∈RK, and pick the one for which the returned classiﬁer-rejector pair yields the lowest balanced error on a validation set. The details are outlined in Algorithm 1, where Λ is the set of multiplier values we wish to search over, and βk = 1 K for the balanced error. In fact, one can further prune the search space by re-parameterizing the rejection criterion in Eq. (13) to search over only K −1 parameters (see Appendix E.1 for details). For problems with two groups (e.g. head and tail), this would mean that we only have a single parameter to tune. For larger number of groups, we prescribe applying an alternate Lagrangian-style min-max optimization to tune the multiplier parameters. We elaborate on this procedure in Appendix E.2. 6
Published as a conference paper at ICLR 2024 Algorithm 1 Cost-sensitive Plug-in (CS-plug-in) 1: Input: Rejection cost c, Group weights β ∈∆L, Pre-trained model p : X →∆L, Sample S 2: Parameters: Multiplier set Λ ⊂RK, Iterations M, Initial prior α(0) ∈(0, 1)K, Initial (ˆh, ˆr) 3: For µ in Λ: 4: For m = 0 to M −1: 5: Construct (h(m+1), r(m+1)) using equations 12–13 with ˆαk = α(m) k βk and ˆµ = µ 6: α(m+1) k = 1 |S| P (x,y)∈S 1(y ∈Gk, r(m+1)(x) = 0), ∀k ∈[K] 7: (ˆh, ˆr) = (h(M), r(M)) if P k βk · ˆek(h(M), r(M)) < P k βk · ˆek(ˆh, ˆr) 8: Return: (ˆh, ˆr) Algorithm 2 Worst-group Plug-in 1: Input: Rejection cost c, Pre-trained model p : X →∆L, Sample Sval divided into S1, S2 2: Parameters: Inital group weights β(0) ∈∆L, Iterations T, Step-size ξ 3: For t = 0 to T 4: (h(t), r(t)) = CS-plug-in(c, β(t), p, S1) 5: β(t+1) k = 1 Z ·β(t) k ·exp(ξ·ˆek(h(t), r(t))), ∀k ∈[K], where Z = P j β(t) j ·exp(ξ·ˆej(h(t), r(t))) 6: Return: (h(T ), r(T )) Figure 2: Plug-in algorithm for the cost-sensitive error P k βk · ˆek(h, r) (Algorithm 1) and worst-group error maxk ˆek(h, r) (Algorithm 2), where ˆek(h, r) = P (x,y)∈S2 1(y̸=h(x),y∈Gk,r(x)=0) P (x,y)∈S2 1(y∈Gk,r(x)=0) . Algorithm 1 can be applied to minimize the balanced L2R risk in Eq. (6) by setting βk = 1/K, ∀k ∈[K].
