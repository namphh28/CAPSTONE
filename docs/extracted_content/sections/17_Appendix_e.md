# Appendix e

E.1). For a target rejection rate, this can be done with a simple percentile computation. For the CSS baseline, we re-train a model for different values of c in {0.0, 0.1, 0.5, 0.75, 0.85, 0.91, 0.95, 0.97, 0.99}. Unfortunately, we ﬁnd that with the Image Net and i Naturalist datasets, the resulting models do not exceed a rejection rate of 40% and 75% respectively, despite us using the entire range of c ∈[0, 1]. F.5 ADDITIONAL PLOTS We present additional plots of the standard 0-1 error as a function of rejection rate for different methods in Figures 4–5. We also present plots of the difference between the tail and head errors as a function of rejection rates in Figures 6–7. F.6 COMPARISON TO EXHAUSTIVE SEARCH OVER α We repeated the CIFAR-100 experiments in §6 choosing the multiplier parameter from a ﬁner grid of values: {0.25, 0.5, 0.75, . . . , 11}. We additionally replaced our heuristic procedure in §4.2 with an exhaustive grid search over group prior α, choosing the head-to-tail ratio from the range {0, 0.01, 0.02, . . . , 1.0}. We pick the parameter combination that yields the lowest balanced error on the held-out validation sample. We show below the balanced error on the test and validation samples. Although the grid search provides gains on the validation set, on the test set, our heuristic approach to picking the multipliers 27
Published as a conference paper at ICLR 2024 and group priors yields metrics that are only slightly worse than grid-search. Moreover, the differences are within standard errors. Test Validation Chow 0.509 ± 0.002 0.498 ± 0.011 Plug-in [Balanced] 0.291 ± 0.008 0.282 ± 0.007 Plug-in [Balanced, grid search] 0.284 ± 0.007 0.264 ± 0.008 28
