# Appendix e

E.1) from three values {1, 6, 11}. For the proposed plug-in method for the worst-group error (Algorithm 2), we set the number of outer iterations T to 25 and the step-size ξ to 1; for the inner call to Algorithm 1, we set M to 10 and tune the parameter ˆλ0 from {1, 6, 11}. We implement the cost-sensitive softmax (CSS) loss of Mozannar & Sontag (2020) described in Equation 3 in their paper. We set the cost vector c(1), . . . , c(L + 1) in this loss to c(i) = 1(y ̸= i), ∀i ∈[L] and c(L + 1) to the rejection cost c. For Chow [Balanced], there exists a plethora of options in the long-tail literature to train the base model (see §2.2 for a brief list of representative approaches). We pick the logit-adjusted cross-entropy loss of Menon et al. (2021a) as a representative approach. For Chow [DRO], as prescribed in Jones et al. (2021), we use the group DRO approach proposed in Sagawa et al. (2020), but modify the inner weighted minimization step to use a logit-adjusted loss (Narasimhan & Menon, 2021; Wang et al., 2023). This modiﬁcation was necessary to adapt the approach of Sagawa et al. (2020) to multi-class classiﬁcation with a large number of classes. Their method requires tuning a step-size for DRO and a regularization parameter C. We pick these parameters from the sets {0.0005, 0.001} and {1.0, 2.0} respectively, by maximizing the worst-group error on the validation sample. F.4 TRADE-OFF PLOTS: FURTHER DETAILS To generate the trade-off plots in Figure 3–7 for each method, we train rejectors with different rejection rates. For this, one may vary the cost parameter c, or equivalently, vary the rejection threshold in the case of Chow’s rule and the threshold ˆτ in the proposed plug-in method (se
