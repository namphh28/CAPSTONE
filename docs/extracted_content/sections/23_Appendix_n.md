# Appendix n

D.3 by setting βk = 1 K , ∀k, and scaling the resulting parameters α∗ y by K. Note that Theorem 12 considers a minimization problem that allows for stochastic classiﬁer-rejector mechanisms. However, since the optimal solution is a deterministic classiﬁer-rejector pair, it still remains the optimal solution when the minimization is over only deterministic classiﬁer-rejector pairs. A.2 PROOF OF LEMMA 2 We prove a more general version of Lemma 2. Consider a variation of Chow’s rule where we modify the base model training to optimize a weighted version of the CE loss in Eq. (3): X k∈[K] βk · E [1(y ∈Gk) · ℓce(y, f(x))] , (15) for some β ∈RK + . One may then use the class probability estimates pwt y (x) ∝exp(f wt y (x)) from the learned scorer f wt to implement Chow’s rule: hwt(x) ∈arg maxy∈[L] pwt y (x); rwt(x) = 1 ⇐⇒maxy∈[L] pwt y (x) < 1 −c. (16) Lemma 5. The variant of Chow’s rule in Eq. (16), where the base model is trained with the balanced cross-entropy loss, results in a classiﬁer and rejector of the form: hwt(x) = arg max y∈[L] β[y] · ηy(x) rbal(x) = 1 ⇐⇒ max y∈[L] β[y] · ηy(x) <  X y′∈[L] β[y′] · ηy′(x)  · (1 −c), (17) where [y] is the index of the group to which class y belongs. Proof. We re-write the weighted CE loss in Eq. (15) as: X y∈[L] E  β[y] · ℓce(y, f(x))  . Since the softmax CE loss is a strictly proper-composite loss, we have that the minimizer pwt y (x) ∝ exp(f wt y (x)) of this objective takes the form: pwt y (x) = 1 Z(x) · β[y] · ηy(x), where Z(x) = P y′ β[y′] · ηy′(x). When applying Chow’s rule to these probabilities, we get: rwt(x) = 1 ⇐⇒ max y pwt y (x) < 1 −c, which is the same as: rwt(x) = 1 ⇐⇒ max y β[y] · ηy(x) <  X y′ β[y′] · ηy′(x)  · (1 −c), as desired. Proof of Lemma 2. The proof follows from Lemma 5 by setting βk = 1 πk . 14
Published as a conference paper at ICLR 2024 A.3 PROOF OF THEOREM 3 Se
