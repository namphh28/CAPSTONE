# Appendix e

D.5. A.4 PROOF OF THEOREM 4 The proof follows from an application of the standard convergence guarantees for the exponentiated gradient descent algorithm (Shalev-Shwartz et al., 2012). One can further bound the estimation errors Ïµgen t in each iteration of Algorithm 2 using standard generalization analysis (Narasimhan et al., 2015a; Tavker et al., 2020; Wang et al., 2023), noting that the classiï¬er and rejectors (ht, rt) are plug-in estimators chosen from a ï¬nite capacity class (namely that of post-hoc linear adjustments to a ï¬xed model p). Proof. Recall that the worst-group L2R risk is given by: Rrej wst(h, r) = max k ek(h, r) + c Â· P(r(x) = 1). This risk can be equivalently re-written as: Rrej wst(h, r) = max Î²âˆˆâˆ†K X k Î²k Â· ek(h, r) + c Â· P(r(x) = 1). We apply standard guarantees for the exponentiated gradient ascent algorithm (Shalev-Shwartz et al., 2012) to treating Ë†e1(h(t), r(t)), . . . , Ë†e K(h(t), r(t)) as the losses incurred by K experts. We have that when Î¾ = q log(K) T , the iterates (h(1), r(1)), . . . , (h(T ), r(T )) in Algorithm 2 satisfy: max Î²âˆˆâˆ†K 1 T T X t=1 X k Î²k Â· Ë†ek(h(t), r(t)) â‰¤1 T T X t=1 X k Î²(t) k Â· Ë†ek(h(t), r(t)) + 2 r log(K) T . Bounding by the estimation error in each iteration, we get: max Î²âˆˆâˆ†K 1 T T X t=1 X k Î²k Â· (ek(h(t), r(t)) âˆ’Ïµgen t ) â‰¤1 T T X t=1 X k Î²(t) k Â· (ek(h(t), r(t)) + Ïµgen t ) + 2 r log(K) T , which is the same as: max Î²âˆˆâˆ†K 1 T T X t=1 X k Î²k Â· ek(h(t), r(t)) â‰¤1 T T X t=1 X k Î²(t) k Â· ek(h(t), r(t)) + 2Â¯Ïµgen + 2 r log(K) T . Adding c Â· P(r(t)(x) = 1) on either side, we have: max Î²âˆˆâˆ†K 1 T T X t=1 X k Î²k Â· ek(h(t), r(t)) + c Â· P(r(t)(x) = 1) â‰¤1 T T X t=1 X k Î²(t) k Â· ek(h(t), r(t)) + c Â· P(r(t)(x) = 1) + 2Â¯Ïµgen + 2 r log(K) T â‰¤1 T T X t=1 inf h,r (X k Î²(t) k Â· ek(h, r) + c Â· P(r(x) = 1) ) + 1 T X t Ïµcs t + 2Â¯Ïµgen + 2 r log(K) T (18) â‰¤inf h,r 1 T T X t=1 X k Î²(t) k Â· ek(h, r) + c Â· P(r(x) = 1) + Â¯Ïµcs + 2Â¯Ïµgen + 2 r log(K) T â‰¤inf h,r X k Â¯Î²k Â· ek(h, r) + c Â· P(r(x) = 1) + Â¯Ïµcs + 2Â¯Ïµgen + 2 r log(K) T (19) 15
Published as a conference paper at ICLR 2024 = inf h,r max Î²âˆˆâˆ†K X k Î²k Â· ek(h, r) + c Â· P(r(x) = 1) + Â¯Ïµcs + 2Â¯Ïµgen + 2 r log(K) T â‰¤inf h,r max k ek(h, r) + c Â· P(r(x) = 1) + Â¯Ïµcs + 2Â¯Ïµgen + 2 r log(K) T = inf h,r Rrej wst(h, r) + Â¯Ïµcs + 2Â¯Ïµgen + 2 r log(K) T , (20) where in Eq. (18), we apply the bound on the excess cost-sensitive risk in line 4 of the algorithm, and in Eq. (19), we denote Â¯Î²k = 1 T PT t=1 Î²(t) k . We can re-write the LHS as: max Î²âˆˆâˆ†K 1 T T X t=1 X k Î²k Â·  ek(h(t), r(t)) + c Â· P(r(t) = 1)  = max Î²âˆˆâˆ†K X k Î²k Â· Etâˆ¼U(1,...,T ) h ek(h(t), r(t)) + c Â· P(r(t) = 1) i = max kâˆˆ[K] Etâˆ¼U(1,...,T ) h ek(h(t), r(t)) + c Â· P(r(t) = 1) i = max kâˆˆ[K] Etâˆ¼U(1,...,T ) h ek(h(t), r(t)) i + c Â· Etâˆ¼U(1,...,T ) h P(r(t) = 1) i Substituting back into Eq. (20) completes the proof. B EQUIVALENCE BETWEEN SELECTIVE CLASSIFICATION AND L2R Selective classiï¬cation seeks to learn a classiï¬er and rejector that minimizes the misclassiï¬cation error with a constraint on the proportion of abstentions: min h,r P (y Ì¸= h(x), r(x) = 0) s.t. P (r(x) = 1) â‰¤b, (21) where b > 0 is an abstention budget. Theorem 6. Suppose P(y | x) and P(x) are continuous in x. For any such abstention budget b > 0 in the constrained problem in Eq. (21), there exists a cost c â‰¥0 cost such that the minimizer of the L2R risk below is also a minimizer for Eq. (21): P (y Ì¸= h(x), r(x) = 0) + c Â· P (r(x) = 1) . The Lagrangian for the constrained problem in Eq. (21) is given by: L(h, r, Î») = P (y Ì¸= h(x), r(x) = 0) + Î» Â· (P(r(x) = 1) âˆ’b) , where Î» â‰¥0 is the Lagrange multiplier. We will ï¬nd it useful to state the following lemma: Lemma 7. Let (hâˆ— Î», râˆ— Î») be the minimizer of L(h, r, Î») for Lagrange multiplier Î» â‰¥0. Then: P (y Ì¸= hâˆ— Î»(x), râˆ— Î»(x) = 0) â‰¤P (y Ì¸= h(x), r(x) = 0) , for all (h, r) such that P(r(x) = 1) â‰¤P(râˆ— Î»(x) = 1). Proof. Since (hâˆ— Î», râˆ— Î») minimizes the Lagrangian, for any (h, r), L(hâˆ— Î», râˆ— Î», Î») â‰¤L(h, r, Î»), i.e., P (y Ì¸= hâˆ— Î»(x), râˆ— Î»(x) = 0) â‰¤P (y Ì¸= h(x), r(x) = 0) + Î» Â· (P(r(x) = 1) âˆ’P(râˆ— Î»(x) = 1)) . Since Î» â‰¥0, for any (h, r) such that P(r(x) = 1) â‰¤P(râˆ— Î»(x) = 1), P (y Ì¸= hâˆ— Î»(x), râˆ— Î»(x) = 0) â‰¤P (y Ì¸= h(x), r(x) = 0) , as desired. We are now ready to prove Theorem 6. 16
Published as a conference paper at ICLR 2024 Proof of Theorem 6. For a ï¬xed Î» â‰¥0, the following is a minimizer of the Lagrangian L(h, r, Î»): hâˆ— Î»(x) âˆˆarg max yâˆˆ[L] P(y | x); râˆ— Î»(x) = 1 â‡â‡’ max yâˆˆ[L] P(y | x) < 1 âˆ’Î». The abstention rate for râˆ— Î»(x) can then be written as: P(râˆ— Î»(x) = 1) = Z maxyâˆˆ[L] P(y|x)<1âˆ’Î» P(x) dx. Since P(y | x) and P(x) are continuous in x, we can always ï¬nd a Î» â‰¥0 for which P(râˆ— Î»(x) = 1) = b. Applying Lemma 7 with this choice of Î», we then have that P (y Ì¸= hâˆ— Î»(x), râˆ— Î»(x) = 0) â‰¤P (y Ì¸= h(x), r(x) = 0) , for all (h, r) such that P(r(x) = 1) â‰¤b. Setting c = Î» completes the proof. C CONDITIONAL LEARNING TO REJECT Geifman & El-Yaniv (2017) minimize a variant of the L2R risk in Eq. (1), where the misclassiï¬cation error is conditioned on samples that are not rejected: Rrej cond(h, r) = P (y Ì¸= h(x) | r(x) = 0) + c Â· P (r(x) = 1) . (22) We show below that the optimal classiï¬er and rejector for this conditional L2R risk continues to have the same form as Eq. (2), but uses a different (distribution-dependent) rejection threshold: Lemma 8. Under Assumption 1, the Bayes-optimal solution to Eq. (22) is given by: hâˆ—(x) = argmax yâˆˆ[L] Î·y(x); râˆ—(x) = 1 â‡â‡’ max yâˆˆ[L] Î·y(x) < 1 âˆ’câ€², (23) for some distribution-dependent threshold câ€² âˆˆ[0, 1]. Proof. The conditional L2R risk in Eq. (22) can be expressed as: Rrej cond(h, r) = P (y Ì¸= h(x), r(x) = 0) 1 âˆ’P (r(x) = 1) + c Â· P (r(x) = 1) . Minimizing this risk can be posed as an equivalent constrained optimization problem: min h,r,Î±âˆˆ(0,1] 1 Î± Â· P (y Ì¸= h(x), r(x) = 0) + c Â· P (r(x) = 1) s.t. P (r(x) = 1) = 1 âˆ’Î±. In particular, there exists an Î±âˆ—âˆˆ(0, 1], such that the minimizer of the following constrained problem also minimizes Eq. (22): min h,r 1 Î±âˆ—Â· P (y Ì¸= h(x), r(x) = 0) + c Â· P (r(x) = 1) s.t. P (r(x) = 1) = 1 âˆ’Î±âˆ— (24) Using arguments similar to the Neyman-Pearson lemma (Neyman & Pearson, 1933), one can show that, under Assumption 1, there exists a Lagrange multiplier Âµâˆ—âˆˆR such that the minimizer to the following problem is also a minimizer to Eq. (24): min h,r 1 Î±âˆ—Â· P (y Ì¸= h(x), r(x) = 0) + (c + Âµâˆ—) Â· P (r(x) = 1) , or equivalently: min h,r Ex  1 Î±âˆ—Â· (1 âˆ’Î·h(x)(x)) Â· 1 (r(x) = 0) + (c + Âµâˆ—) Â· 1 (r(x) = 1)  . The following are then minimizers of the above objective: hâˆ—(x) âˆˆarg max yâˆˆ[L] Î·y(x) and râˆ—(x) = 1 â‡â‡’ max yâˆˆ[L] Î·y(x) < 1 âˆ’Î±âˆ—Â· (c + Âµâˆ—). Setting câ€² = Î±âˆ—Â· (c + Âµâˆ—) completes the proof. 17
Published as a conference paper at ICLR 2024 D BAYES-OPTIMAL CLASSIFIERS AND REJECTORS We derive the Bayes-optimal classiï¬er and rejector for the balanced L2R risk (6) and the generalized L2R risk (14). We will will ï¬rst consider a general formulation where we allow stochastic classiï¬er- rejector mechanisms and then specialize to the case of deterministic classiï¬ers and rejectors. D.1 CONFUSION STATISTICS We begin with some useful deï¬nitions. Suppose Y = [n] = {1, . . . , n}. We deï¬ne the confusion matrix for a classiï¬er-rejector pair (h, r) by: Cij(h, r) = E(x,y)âˆ¼P [1 (y = i, h(x) = j, r(x) = 0)] We allow for stochastic mechanisms F that are deï¬ned by a distribution over classiï¬er-rejector pairs. We adopt the convention in Narasimhan et al. (2015a) and deï¬ne the confusion matrix for a stochastic mechanism F by: Cij(F) = E(h,r)âˆ¼F [Cij(h, r)] . We will also ï¬nd it useful to deï¬ne the space of all confusion matrices realizable by distributions over classiï¬ers-rejector pairs: C =  C(F) âˆˆ[0, 1]nÃ—n âˆ€distributions F over (h, r) . It is easy to show that the set C is convex. Lemma 9. C is a convex set. Proof. For any C(1), C(2) âˆˆC and Î³ âˆˆ[0, 1], we will show Î³ Â· C(1) + (1 âˆ’Î³) Â· C(2) âˆˆC. Clearly, there exists stochastic mechanisms F1, F2 such that C(1) = C(F1) and C(2) = C(F2). Since F â€² = Î³Â·F1+(1âˆ’Î³)Â·F2 represents a valid stochastic mechanism, C(F â€²) = Î³Â·C(1)+(1âˆ’Î³)Â·C(2) âˆˆC. D.2 OPTIMAL SOLUTIONS TO LINEAR MINIMIZATION OVER C We ï¬rst state a couple of helper lemmas that characterize the optimal solutions to linear minimization problems over the set C. Suppose we wish to minimize the following objective over stochastic mechanisms F: min F X ij Wij Â· Cij[F], for coefï¬cients W âˆˆRLÃ—L. One may equivalently formulate this as a linear minimization over C: min CâˆˆC X ij Wij Â· Cij. (25) Lemma 10. Suppose Assumption 1 holds. Let W âˆˆRLÃ—L be a coefï¬cient matrix such that no two columns are identical. Then Eq. (25) admits a unique solution. Furthermore, the minimizing confusion matrix is achieved by the following deterministic classiï¬er-rejector pair: hâˆ—(x) âˆˆarg min yâˆˆ[L] X i Wi,y Â· Î·i(x); râˆ—(x) = 1 â‡â‡’ min yâˆˆ[L] X i Wi,y Â· Î·i(x) > 0. Proof. The ï¬rst part (uniqueness claim) directly follows from Lemma 24 in Narasimhan et al. (2022b). For the second part, we re-write Eq. (25) into an minimization problem over stochastic mechanisms F: min CâˆˆC X i,j Wij Â· Cij = min F X i,j Wij Â· Cij(F) = min F E(h,r)âˆ¼F h X i,j Wij Â· Cij(h, r) i 18
Published as a conference paper at ICLR 2024 = min (h,r) X i,j Wij Â· Cij(h, r) = min (h,r) Ex h X i,j Wij Â· P(y = i, h(x) = j, r(x) = 0) i = min (h,r) Ex h X i,j Wij Â· Î·i(x) Â· 1(h(x) = j, r(x) = 0) i , (26) where the third step follows from the fact that for any function Ï•, the expectation E(h,r)âˆ¼F [Ï•(h, r)] is minimized by a point mass on arg min(h,r) Ï•(h, r). The minimizer of Eq. (26) minimizes the term within the expectation pointwise: for any given x, (hâˆ—(x), râˆ—(x)) âˆˆ arg min yâˆˆ[L],zâˆˆ{0,1} X i Wiy Â· Î·i(x) Â· 1(z = 0), which takes the form in the statement of the lemma. For our next helper lemma, we consider a constrained minimization problem over stochastic mecha- nisms F: min F X i,j Wij Â· Cij[F] s.t. X i,j w(k) i Â· Cij[F] = bk, âˆ€k âˆˆ[K], for coefï¬cients W âˆˆRLÃ—L and w(1), . . . , w(K) âˆˆRL. Once again, we may formulate this as an equivalent constrained linear minimization over C: min CâˆˆC X i,j Wij Â· Cij s.t. X i,j w(k) i Â· Cij = bk, âˆ€k âˆˆ[K]. (27) Lemma 11. Suppose Assumption 1 holds. Let W âˆˆRLÃ—L be a coefï¬cient matrix such that no two columns are identical. Let w(1), . . . , w(K) âˆˆRL be constraint coefï¬cients so that Eq. (27) admits a feasible solution. Then there exists multipliers Âµâˆ—âˆˆRK such that the minimizer to Eq. (27) coincides with the minimizer to the following unconstrained linear minimization over C: min CâˆˆC X i,j  Wij âˆ’ X kâˆˆ[K] Âµâˆ— k Â· w(k) i  Â· Cij. (28) Proof. We ï¬rst write the Lagrangian for Eq. (27): L(C, Âµ) = X i,j Wij Â· Cij âˆ’ X kâˆˆ[K] Âµk Â·  X i,j w(k) i Â· Cij âˆ’bk  , (29) where Âµ1, . . . , ÂµK âˆˆR are multipliers associated with the K constraints. Since the objective and constraints in Eq. (27) are linear in C, and C is convex from Lemma 9, Eq. (27) is a convex minimization problem. Furthermore, since it has at least one feasible solution, Slaterâ€™s condition is satisï¬ed, and strong duality holds. As a result, there exists an optimal primal solution Câˆ—that satisï¬es the constraints in Eq. (27), and optimal dual solution Âµâˆ—such that: L(Câˆ—, Âµâˆ—) = min CâˆˆC L(C, Âµâˆ—) = max ÂµâˆˆRK L(Câˆ—, Âµ) = X i,j Wij Â· Câˆ— ij. (30) Furthermore, we have that: min CâˆˆC L(C, Âµâˆ—) = min CâˆˆC X i,j  Wij âˆ’ X kâˆˆ[K] Âµâˆ— k Â· w(k) i  Â· Cij + Ï‰âˆ—= min CâˆˆC X i,j Lij Â· Cij + Ï‰âˆ—, where Lij = Wij âˆ’P kâˆˆ[K] Âµâˆ— k Â· w(k) i and Ï‰âˆ—= P kâˆˆ[K] Âµâˆ— k Â· bk is independent of C. Observe that the coefï¬cient matrix L on the right-hand side has no two columns identical. To see this, each column of L is of the form L:,j = W:,j âˆ’P kâˆˆ[K] Âµâˆ— k Â· w(k), where w(k) = [w(k) 1 , . . . , w(k) L ]âŠ¤. Since W has no two columns identical, i.e., W:j Ì¸= W:jâ€², âˆ€j Ì¸= jâ€², we have that L:,j Ì¸= L:,jâ€². Using Assumption 1 and the above observation, we can apply the ï¬rst part of Lemma 10 to show that min CâˆˆC L(C, Âµâˆ—) has a unique solution. Since L(Câˆ—, Âµâˆ—) = min CâˆˆC L(C, Âµâˆ—) from Eq. (30), we have that the optimal primal solution Câˆ—is a unique minimizer to min CâˆˆC L(C, Âµâˆ—). In other words, Câˆ—is the unique minimizer to the unconstrained linear problem in Eq. (28). 19
Published as a conference paper at ICLR 2024 D.3 BAYES-OPTIMAL CLASSIFIER AND REJECTOR FOR COST-SENSITIVE L2R In this section, we derive the Bayes-optimal stochastic mechanism that minimizes the following cost-sensitive objective, with a penalty of c > 0 for abstention. Speciï¬cally, deï¬ne: Rrej cs (h, r) = X kâˆˆ[K] Î²k Â· P  y Ì¸= h(x) r(x) = 0, y âˆˆGk  + c Â· P (r(x) = 1) , (31) where Î² âˆˆâˆ†K are the costs associated with the K groups. One can re-deï¬ne Eq. (31) in terms of the confusion matrices C(h, r) as follows: Rrej cs (h, r) = X kâˆˆ[K] Î²k Â· P  y Ì¸= h(x), r(x) = 0, y âˆˆGk  P  r(x) = 0, y âˆˆGk  + c Â· P (r(x) = 1) = X kâˆˆ[K] Î²k Â· P iâˆˆGk P jÌ¸=i Cij(h, r) P iâˆˆGk P jâˆˆ[L] Cij(h, r) + c Â·  1 âˆ’ X i,j Cij(h, r)  . For a stochastic mechanism F, we can similarly deï¬ne Rrej cs (F) = X kâˆˆ[K] Î²k Â· P iâˆˆGk P jÌ¸=i Cij(F) P iâˆˆGk P jâˆˆ[L] Cij(F) + c Â·  1 âˆ’ X i,j Cij(F)  . (32) We derive an optimal stochastic classiï¬er that minimizes Eq. (32): Theorem 12. Under Assumptions 1 and 2, there exists group-speciï¬c parameters Î±âˆ—âˆˆ(0, 1)K and Âµâˆ—âˆˆRK such that the following deterministic classiï¬er-rejector pair is an optimal solution to Eq. (32): hâˆ—(x) = arg max yâˆˆ[L] Î²[y] Î±âˆ— [y] Â· Î·y(x); râˆ—(x) = 1 â‡â‡’ max yâˆˆ[L] Î²[y] Î±âˆ— [y] Â· Î·y(x) < X yâ€²âˆˆ[L] Î²[yâ€²] Î±âˆ— [yâ€²] âˆ’Âµâˆ— [yâ€²] ! Â· Î·yâ€²(x) âˆ’c, (33) where [y] is the index of the group class y belongs to. Furthermore, Î±âˆ— k = P (râˆ—(x) = 0, y âˆˆGk). Proof. From the boundedness condition in Assumption 2, we know that there exists Î±âˆ—âˆˆ(0, 1)K such that the minimizer to the following problem also minimizes Eq. (32): min F X kâˆˆ[K] Î²k Î±âˆ— k Â· X iâˆˆGk X jÌ¸=i Cij(F) + c Â·  1 âˆ’ X i,j Cij(F)  (34) s.t. X iâˆˆGk X jâˆˆ[L] Cij(F) = Î±âˆ— k, âˆ€k âˆˆ[K]. One may equivalently formulate a constrained linear minimization problem over C: min CâˆˆC X kâˆˆ[K] Î²k Î±âˆ— k Â· X iâˆˆGk X jÌ¸=i Cij + c Â·  1 âˆ’ X i,j Cij  (35) s.t. X iâˆˆGk X jâˆˆ[L] Cij = Î±âˆ— k, âˆ€k âˆˆ[K]. One can now directly apply the result in Lemma 11 with Wij = P kâˆˆ[K] Î²k Î±âˆ— k Â· 1(i âˆˆGk, j Ì¸= i) âˆ’c and w(k) i = 1(i âˆˆGk), âˆ€i âˆˆ[L], noting that no two columns of W are identical. Consequently, we have that there exists multipliers Âµâˆ—âˆˆRK such that the minimizer to the following unconstrained linear minimization over C also minimizes Eq. (35): min CâˆˆC X i,j  Wij âˆ’ X kâˆˆ[K] Âµâˆ— k Â· w(k) i  Â· Cij. 20
Published as a conference paper at ICLR 2024 Applying Lemma 10, we have that the above problem admits a unique minimizer Câˆ—which is achieved by the following deterministic classiï¬er-rejector pair: hâˆ—(x) âˆˆarg min yâˆˆ[L] X i  Wiy âˆ’ X kâˆˆ[K] Âµâˆ— k Â· w(k) i  Â· Î·i(x); râˆ—(x) = 1 â‡â‡’ min yâˆˆ[L] X i  Wiy âˆ’ X kâˆˆ[K] Âµâˆ— k Â· w(k) i  Â· Î·i(x) > 0. Substituting for W and w(k) i then gives us: hâˆ—(x) âˆˆarg min yâˆˆ[L] X kâˆˆ[K] X iâˆˆGk  Î²k Î±âˆ— k Â· 1(y Ì¸= i) âˆ’Âµâˆ— k  Â· Î·i(x) âˆ’c; râˆ—(x) = 1 â‡â‡’ min yâˆˆ[L] X kâˆˆ[K] X iâˆˆGk  Î²k Î±âˆ— k Â· 1(y Ì¸= i) âˆ’Âµâˆ— k  Â· Î·i(x) > c. The optimal classiï¬er hâˆ—then simpliï¬es to: hâˆ—(x) âˆˆarg max yâˆˆ[L] X kâˆˆ[K] X yâˆˆGk Î²k Î±âˆ— k Â· Î·y(x). Since each class y belongs exactly one group, this is the same as: hâˆ—(x) âˆˆarg max yâˆˆ[L] Î²[y] Î±âˆ— [y] Â· Î·y(x), where [y] is the index of the group class y belongs to. Similarly, the optimal rejector râˆ—simpliï¬es to râˆ—(x) = 1 â‡â‡’ min yâˆˆ[L] X kâˆˆ[K] X iâˆˆGk  Î²k Î±âˆ— k âˆ’Âµâˆ— k  Â· Î·i(x) âˆ’ X kâˆˆ[K] X yâˆˆGk Î²k Î±âˆ— k Â· Î·y(x) > c, which is the same as: râˆ—(x) = 1 â‡â‡’ max yâˆˆ[L] Î²[y] Î±âˆ— [y] Â· Î·y(x) < X yâ€²âˆˆ[L]  Î²[yâ€²] Î±âˆ— [yâ€²] âˆ’Âµâˆ— [yâ€²]  Â· Î·yâ€²(x) âˆ’c, as desired. D.4 SPECIAL CASE OF BINARY GROUPS WITH BINARY LABELS In the special case where we have binary labels, each in a separate group, the rejector in Theorem 12 takes a much simpler form, with a constant threshold for each class. Corollary 13. Suppose Y = G = {0, 1}. Under Assumption 1, the optimal classiï¬er and rejector for Eq. (32) simpliï¬es to applying class-speciï¬c thresholds Ï„ âˆ— 0 , Ï„ âˆ— 1 âˆˆR and Î³âˆ—âˆˆ(0, 1): hâˆ—(x) = 1 (Î·1(x) > Î³âˆ—) ; râˆ—(x) = 1 â‡â‡’ Î·hâˆ—(x)(x) < Ï„ âˆ— hâˆ—(x). Proof. When Y = G = {0, 1}, we have from Theorem 12 that the optimal classiï¬er takes the form: hâˆ—(x) = arg max  Î²0 Î±âˆ— 0 Â· Î·0(x), Î²1 Î±âˆ— 1 Â· Î·1(x)  = 1  Î²0 Î±âˆ— 0 Â· Î·0(x) < Î²1 Î±âˆ— 1 Â· Î·1(x)  = 1 (Î·1(x) > Î³âˆ—) , where Î³âˆ—= Î²0/Î±âˆ— 0 Î²0/Î±âˆ— 0+Î²1/Î±âˆ— 1 , as desired; the last equality uses the fact that Î·0(x) + Î·1(x) = 1. Similarly, the optimal rejector is given by: râˆ—(x) = 1 â‡â‡’ Î²hâˆ—(x) Î±âˆ— hâˆ—(x) Â· Î·hâˆ—(x)(x) <  Î²0 Î±âˆ— 0 âˆ’Âµâˆ— 0  Â· Î·0(x) +  Î²1 Î±âˆ— 1 âˆ’Âµâˆ— 1  Â· Î·1(x) âˆ’c. (36) 21
Published as a conference paper at ICLR 2024 Consider the case when hâˆ—(x) = 1. We can then use the fact that Î·0(x) + Î·1(x) = 1 to re-write the right-hand side of Eq. (36) as: Î²1 Î±âˆ— 1 Â· Î·1(x) <  Î²0 Î±âˆ— 0 âˆ’Âµâˆ— 0  Â· (1 âˆ’Î·1(x)) +  Î²1 Î±âˆ— 1 âˆ’Âµâˆ— 1  Â· Î·1(x) âˆ’c, which simpliï¬es to: Î·1(x) < Ï„ âˆ— 1 , where Ï„ âˆ— 1 = Î²0 Î±âˆ— 0 âˆ’Âµâˆ— 0 âˆ’c Î²0 Î±âˆ— 0 + Âµâˆ— 1 âˆ’Âµâˆ— 0 . Similarly, when hâˆ—(x) = 1, the right-hand side of Eq. (36) simpliï¬es to: Î·0(x) < Ï„ âˆ— 0 , where Ï„ âˆ— 0 = Î²1 Î±âˆ— 1 âˆ’Âµâˆ— 1 âˆ’c Î²1 Î±âˆ— 1 + Âµâˆ— 0 âˆ’Âµâˆ— 1 , which completes the proof. D.5 BAYES-OPTIMAL CLASSIFIER AND REJECTOR FOR GENERALIZED L2R We next derive the Bayes-optimal classiï¬er and rejector for the generalized L2R risk in Eq. (14), which we restate below: Rrej gen(h, r) = Ïˆ (e1(h, r), . . . , e K(h, r)) + c Â· P (r(x) = 1) , (37) where Ïˆ : [0, 1]K â†’R+ is a general function, and encompasses several common metrics (Lipton et al., 2014; Menon et al., 2013; Narasimhan et al., 2015a). One may equivalently write Eq. (37) in terms of the confusion matrix C(h, r): Rrej gen(h, r) = Ïˆ  P iâˆˆG1 P jÌ¸=i Cij(h, r) P iâˆˆG1 P jâˆˆ[L] Cij(h, r), . . . , P iâˆˆGK P jÌ¸=i Cij(h, r) P iâˆˆGK P jâˆˆ[L] Cij(h, r)  + c Â·  1 âˆ’ X i,j Cij(h, r)  . Allowing for stochastic mechanisms F that are deï¬ned by a distribution over classiï¬er-rejector pairs, we would like to minimize: min F Ïˆ  P iâˆˆG1 P jÌ¸=i Cij(F) P iâˆˆG1 P jâˆˆ[L] Cij(F), . . . , P iâˆˆGK P jÌ¸=i Cij(F) P iâˆˆGK P jâˆˆ[L] Cij(F)  + c Â·  1 âˆ’ X i,j Cij(F)  , or equivalently: min F Ïˆ Ï†1(C(F)) Ï€1(C(F)), . . . , Ï†K(C(F)) Ï€K(C(F))  + c Â·  1 âˆ’ X k Ï€k(C(F))  , (38) where Ï†k(C) = P iâˆˆGk P jÌ¸=i Cij(F) and Ï€k(C) = P iâˆˆGk P jâˆˆ[L] Cij(F). Theorem 7 (restated). For any non-linear Ïˆ, there exists coefï¬cients u(i), v(i) âˆˆRK, i âˆˆ[K + 1] and a parameter Î½ âˆˆâˆ†K+1, such that, the generalized L2R in Eq. (14) is minimized by a stochastic mechanism that for any x, predicts using (h(i), r(i)) with probability Î½i: h(i)(x) âˆˆarg max y u(i) y Â· Î·y(x), i âˆˆ[K + 1]; r(i)(x) = 1 â‡â‡’ max y u(i) y Â· Î·y(x) < X yâ€² v(i) yâ€² Â· Î·yâ€²(x) âˆ’c, i âˆˆ[K + 1]. 22
Published as a conference paper at ICLR 2024 Proof. One may reformulate Eq. (38) as an optimization over the space of all confusion statistics realizable through distribution over classiï¬ers-rejector pairs C, or equivalently over a smaller space of transformed confusion matrices: Câ€² = {(Ï†, Ï€), where Ï† = (Ï†1(C), . . . , Ï†K(C)), and Ï€ = (Ï€1(C), . . . , Ï€K(C)) | âˆ€C âˆˆC}. We can then solve Eq. (38) by solving: min (Ï†,Ï€)âˆˆCâ€² Ïˆ Ï†1 Ï€1 , . . . , Ï†K Ï€K  + c Â·  1 âˆ’ X k Ï€k  . (39) We know from Lemma 9 that C is convex; consequently, it is easy to see that Câ€² is also convex. Therefore any point (Ï†, Ï€) âˆˆCâ€² can be expressed as a convex combination of extreme points of Câ€². In fact, following Proposition 10 in Narasimhan et al. (2022b), any (Ï†, Ï€) âˆˆCâ€² can be expressed as convex combination of K + 1 extreme points (Ï†(1), Ï€(1)), . . . (Ï†(K+1), Ï€(K+1)), each of which is a unique linear minimizer over Câ€²: (Ï†(i), Ï€(i)) âˆˆarg min (Ï†,Ï€)âˆˆCâ€² X k w(i) k Â· Ï†k + X k q(i) k Â· Ï€k for coefï¬cients w(i), q(i) âˆˆRK, i âˆˆ[K + 1]. Therefore this is also true for the minimizer (Ï†âˆ—, Ï€âˆ—) âˆˆCâ€² of Eq. (39). All that remains to show is that these K +1 linear minimizers are realized by K +1 classiï¬er-rejector pairs of the form in the theorem statement. To show this, we note that for coefï¬cients w(i), q(i) âˆˆRK, minimizing the linear objective over Câ€²: min (Ï†,Ï€)âˆˆCâ€² X k w(i) k Â· Ï†k + X k q(i) k Â· Ï€k is equivalent to minimizing the same objective over the boundary points of Câ€², and equivalently, over deterministic classiï¬er-rejector pairs: min h,r X k w(i) k Â· Ï†k(C(h, r)) + X k q(i) k Â· Ï€k(C(h, r)). (40) Expanding the above objective, we get: X k w(i) k Â· E [1(y Ì¸= h(x), r(x) = 0, y âˆˆGk)] + X k q(i) k Â· E [1(r(x) = 0, y âˆˆGk)] , which is the same as: Ex ï£® ï£°X k w(i) k X yâˆˆGk Î·y(x) Â· 1(y Ì¸= h(x), r(x) = 0) + X k q(i) k X yâˆˆGk Î·y(x) Â· 1(r(x) = 0) ï£¹ ï£», or: Ex  X y w(i) [y] Â· Î·y(x) âˆ’w(i) [h(x)] Â· Î·h(x)(x) + X y q(i) [y] Â· Î·y(x)  Â· 1(r(x) = 0)  . The minimizer of Eq. (40) then takes the form: h(i)(x) âˆˆarg max yâˆˆ[L] w(i) [y] Â· Î·y(x); r(i)(x) = 1 â‡â‡’ max yâˆˆ[L] w(i) [y] Â· Î·y(x) < X yâˆˆ[L] (w(i) [y] + q(i) [y]) Â· Î·y(x). Setting u(i) y = w(i) [y] and v(i) y = w(i) [y] + q(i) [y] + c, the above is equivalent to: h(i)(x) âˆˆarg max yâˆˆ[L] u(i) y Â· Î·y(x); r(i)(x) = 1 â‡â‡’ max yâˆˆ[L] u(i) y Â· Î·y(x) < X yâˆˆ[L] v(i) y Â· Î·y(x) âˆ’c. It thus follows that the minimizer (Ï†âˆ—, Ï€âˆ—) = (Ï†(C(F âˆ—)), Ï€(C(F âˆ—)) to Eq. (39) is real- ized by a stochastic mechanism F âˆ—that randomizes over the K + 1 classiï¬er-rejector pairs (h(1), r(1)), . . . , (h(K+1), r(K+1)). 23
Published as a conference paper at ICLR 2024 Algorithm 3 Lagrangian-based Plug-in for Balanced Error 1: Input: Rejection cost c, Pre-trained p : X â†’âˆ†L, Sample S 2: Parameters: Iterations M, T, Step-sizes Î¾Âµ, Î¾Î±, Initial Î±(0) âˆˆ(0, K)K, Lower bound Îº > 0 3: For m = 0 to M 4: h(m)(x) = arg maxyâˆˆ[L] u(m) y Â· py(x), where u(m) y = 1 Î±(m) [y] 5: Initialize Âµ(m,0) âˆˆRK 6: For t = 0 to T âˆ’1 7: Âµ(m,t+1) k = Âµ(m,t) k + Î¾Âµ Â·  Î±(m) i âˆ’K |S| P (x,y)âˆˆS 1 (r(x) = 0, y âˆˆGk)  , âˆ€k âˆˆ[K] 8: r(m,t+1)(x) = 1 â‡â‡’ maxyâˆˆ[L] u(m) y Â· py(x) < P yâ€²âˆˆ[L] v(m) yâ€² Â· pyâ€²(x) âˆ’c, 9: where v(m) y = 1 Î±(m) [y] âˆ’Âµ(m,t+1) [y] . 10: End For 11: Î±(m+1) k = Î±(m) k âˆ’Î¾Î± Â·  Âµ(m,T ) k âˆ’ 1 (Î±(m) k )2 Â· Ë†ek(h(m), r(m,T ))  12: Î±(m+1) k = proj[Îº, Kâˆ’Îº]  Î±(m) k  , where proj[a,b](z) = max{min{z, b}, a} 13: End For 14: Return: (h(M), r(M,T )) E PLUG-IN APPROACH: FURTHER DETAILS We provide further details about the plug-in approach in Â§4. E.1 RE-PARAMETERIZING THE REJECTOR FOR THE BALANCED ERROR One can further prune the search space over multipliers Ë†Âµ by re-parameterizing the rejection criterion in Eq. (13) to search over only K âˆ’1 parameters. To this end, using the fact that P yâˆˆ[L] py(x) = 1, we re-write the criterion as: max yâˆˆ[L] 1 Ë†Î±[y] Â· py(x) < X yâ€²âˆˆ[L] 1 Ë†Î±[yâ€²] Â· pyâ€²(x) âˆ’ X yâ€²âˆˆ[L] (Ë†Âµ[yâ€²] âˆ’Ë†ÂµK) Â· pyâ€²(x) âˆ’Ë†ÂµK X yâ€²âˆˆ[L] pyâ€²(x) âˆ’c, which is the same as: max yâˆˆ[L] 1 Ë†Î±[y] Â· py(x) < X yâ€²âˆˆ[L] 1 Ë†Î±[yâ€²] Â· pyâ€²(x) âˆ’ X kâˆˆ[Kâˆ’1] (Ë†Âµk âˆ’Ë†ÂµK) Â· X yâ€²âˆˆGk pyâ€²(x) âˆ’Ë†ÂµK âˆ’c. This can be equivalently re-paramterized as: max yâˆˆ[L] 1 Ë†Î±[y] Â· py(x) < X yâ€²âˆˆ[L] 1 Ë†Î±[yâ€²] Â· pyâ€²(x) âˆ’ X kâˆˆ[Kâˆ’1] X yâ€²âˆˆGk Ë†Î»k Â· pyâ€²(x) âˆ’Ë†Ï„, where Ë†Î»k = Ë†Âµk âˆ’Ë†ÂµK, for k âˆˆ[K âˆ’1], and Ë†Ï„ = Ë†ÂµK âˆ’c. Note that in practice, one is often prescribed a target rejection rate, and picks the rejection cost c to achieve this rejection rate. For ï¬xed Ë†Î»1, . . . , Ë†Î»Kâˆ’1, one may equivalently pick the threshold Ë†Ï„ (instead of cost c) to reach the desired rejection rate. So given a target rejection rate, all one needs to do is to apply a brute-force search to tune the remaining K âˆ’1 unknowns to minimize the balanced error on a validation set, with the threshold Ë†Ï„ set to a percentile that matches the desired rejection rate. When K = 2 (head and tail), i.e., G = {0, 1}, this would mean that we only have a single parameter Ë†Î»0 = Ë†Âµ0 âˆ’Ë†Âµ1 to tune, which can be efï¬ciently done using a simple line search. E.2 LAGRANGIAN-BASED PLUG-IN APPROACH As an alternative to plug-in approach in Algorithm 1, we explore a Lagrangian-based approach that handles the constraints more intricately. We begin by introducing multipliers Âµ âˆˆRK for the K 24
Published as a conference paper at ICLR 2024 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 Standard Error Chow CSS Chow [BCE] Plug-in [Balanced] (a) CIFAR-100 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.1 0.2 0.3 0.4 Standard Error Chow CSS Chow [BCE] Plug-in [Balanced] (b) Image Net 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Standard Error Chow CSS Chow [BCE] Plug-in [Balanced] (c) i Naturalist Figure 4: Balanced L2R: Standard 0-1 error as a function of proportion of rejections. 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.1 0.2 0.3 0.4 0.5 Standard Error Chow CSS Chow [DRO] Plug-in [Worst] (a) CIFAR-100 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.1 0.2 0.3 0.4 Standard Error Chow CSS Chow [DRO] Plug-in [Worst] (b) Image Net 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Standard Error Chow CSS Chow [DRO] Plug-in [Worst] (c) i Naturalist Figure 5: Worst-group L2R: Standard 0-1 error as a function of proportion of rejections. constraints in the optimization problem in Eq. (7) and writing out the Lagrangian: L(h, r, Î±; Âµ) = X kâˆˆ[K] 1 Î±k Â· P (y Ì¸= h(x), r(x) = 0, y âˆˆGk) + c Â· P (r(x) = 1) âˆ’ X kâˆˆ[K] Âµk Â· (K Â· P (r(x) = 0, y âˆˆGk) âˆ’Î±k) , and formulate an equivalent saddle point optimization problem: min Î±,h,r max ÂµâˆˆRK L(h, r, Î±; Âµ). (41) For a ï¬xed Î±, we can solve the min-max problem over h, r and Âµ by alternating between gradient- ascent updates on the multipliers Âµ: Âµ(t+1) k = Âµ(t) k + Î¾Âµ Â· (Î±k âˆ’K Â· P (r(x) = 0, y âˆˆGk)) , âˆ€k âˆˆ[K], for step size Î¾Âµ > 0, and a full minimization over h and r. The procedure outlined in Algorithm 3 then solves the saddle point optimization problem in Eq. (41) by performing gradient-descent on Î±, and the combination of the gradient-ascent and full minimization procedure described above over h, r and Âµ. It is worth noting that this Lagrangian procedure is made possible by viewing Eq. (7) as an opti- mization problem over the space of the space of realizable confusion statistics C (Narasimhan, 2018; Narasimhan et al., 2022b), which we can then show is a continuous constrained optimization problem that is amenable be solved with Lagrangian style updates. 25
Published as a conference paper at ICLR 2024 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.2 0.4 0.6 0.8 1.0 Tail Error - Head Error Chow CSS Chow [BCE] Plug-in [Balanced] (a) CIFAR-100 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Tail Error - Head Error Chow CSS Chow [BCE] Plug-in [Balanced] (b) Image Net 0.00 0.25 0.50 0.75 Proportion of Rejections âˆ’0.05 0.00 0.05 0.10 Tail Error - Head Error Chow CSS Chow [BCE] Plug-in [Balanced] (c) i Naturalist Figure 6: Balanced L2R: Difference between tail and head errors as a function of proportion of rejections. 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.2 0.4 0.6 0.8 1.0 Tail Error - Head Error Chow CSS Chow [DRO] Plug-in [Worst] (a) CIFAR-100 0.0 0.2 0.4 0.6 0.8 Proportion of Rejections 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Tail Error - Head Error Chow CSS Chow [DRO] Plug-in [Worst] (b) Image Net 0.00 0.25 0.50 0.75 Proportion of Rejections âˆ’0.025 0.000 0.025 0.050 0.075 0.100 0.125 Tail Error - Head Error Chow CSS Chow [DRO] Plug-in [Worst] (c) i Naturalist Figure 7: Worst-group L2R: Difference between tail and head errors as a function of proportion of rejections. F ADDITIONAL EXPERIMENTAL RESULTS We discuss the data splits, hyper-parameter choices and further implementation details in our exper- iments. We also include additional experimental plots. All results are averaged over 5 trials. We report 95% conï¬dence intervals in Table 2. F.1 HYPER-PARAMETER CHOICES We apply the same data pre-processing as Menon et al. (2021a). We trained all the models using SGD with a momentum of 0.9 and a weight decay of 10âˆ’4. We summarise the hyper-parameter choices in Table 3 below. Dataset #classes ntrain ntest Tail prop. Model Base LR Schedule Epochs Batch size CIFAR-100-LT 100 50,000 10,000 0.03 CIFAR Res Net-32 0.4 anneal 256 128 Image Net-LT 1000 1,281,167 50,000 0.02 Res Net-50 0.4 cosine 200 512 i Naturalist 2018 8143 437,513 24,426 0.12 Res Net-50 0.4 cosine 200 1024 Table 3: Dataset details and hyper-parameter choices. For CIFAR-100, we apply a warm up with a linear learning rate for 15 steps until we reach the base learning rate. We apply a learning rate decay of 0.1 at the 96th, 192nd and 224th epochs. For Image Net and i Naturalist, we apply a warm up with a linear learning rate for 5 steps until we reach the base learning rate. We apply a learning rate decay of 0.1 at the 45th, 100th and 150th epochs. 26
Published as a conference paper at ICLR 2024 F.2 DATASET SPLITS The train, test and validations samples all follow the same long-tailed label distributions. Each dataset comes with an original test set with equal proportions of all labels. We re-weight the samples in the original test set to replicate the same label proportions as the training set. Furthermore, we hold out 20% of the original test set as a validation sample and use the remaining as the test sample. F.3 IMPLEMENTATION DETAILS For the proposed plug-in method for the balanced error (Algorithm 1), we set the number of iterations M to 10, and tune the single parameter Ë†Î»0 = Ë†Âµ0 âˆ’Ë†Âµ1 (se
