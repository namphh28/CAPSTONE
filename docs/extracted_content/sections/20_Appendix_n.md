# Appendix n

E.2. 6
Published as a conference paper at ICLR 2024 Algorithm 1 Cost-sensitive Plug-in (CS-plug-in) 1: Input: Rejection cost c, Group weights β ∈∆L, Pre-trained model p : X →∆L, Sample S 2: Parameters: Multiplier set Λ ⊂RK, Iterations M, Initial prior α(0) ∈(0, 1)K, Initial (ˆh, ˆr) 3: For µ in Λ: 4: For m = 0 to M −1: 5: Construct (h(m+1), r(m+1)) using equations 12–13 with ˆαk = α(m) k βk and ˆµ = µ 6: α(m+1) k = 1 |S| P (x,y)∈S 1(y ∈Gk, r(m+1)(x) = 0), ∀k ∈[K] 7: (ˆh, ˆr) = (h(M), r(M)) if P k βk · ˆek(h(M), r(M)) < P k βk · ˆek(ˆh, ˆr) 8: Return: (ˆh, ˆr) Algorithm 2 Worst-group Plug-in 1: Input: Rejection cost c, Pre-trained model p : X →∆L, Sample Sval divided into S1, S2 2: Parameters: Inital group weights β(0) ∈∆L, Iterations T, Step-size ξ 3: For t = 0 to T 4: (h(t), r(t)) = CS-plug-in(c, β(t), p, S1) 5: β(t+1) k = 1 Z ·β(t) k ·exp(ξ·ˆek(h(t), r(t))), ∀k ∈[K], where Z = P j β(t) j ·exp(ξ·ˆej(h(t), r(t))) 6: Return: (h(T ), r(T )) Figure 2: Plug-in algorithm for the cost-sensitive error P k βk · ˆek(h, r) (Algorithm 1) and worst-group error maxk ˆek(h, r) (Algorithm 2), where ˆek(h, r) = P (x,y)∈S2 1(y̸=h(x),y∈Gk,r(x)=0) P (x,y)∈S2 1(y∈Gk,r(x)=0) . Algorithm 1 can be applied to minimize the balanced L2R risk in Eq. (6) by setting βk = 1/K, ∀k ∈[K]. 5 EXTENSION TO GENERAL EVALUATION METRICS We next extend our results to evaluation metrics more general than the balanced error. To this end, we denote the conditional error of classiﬁer h and rejector r on group Gk by: ek(h, r) = P (y ̸= h(x) | r(x) = 0, y ∈Gk) . We then consider the problem of minimizing a general function ψ : [0, 1]K →R of the per-group errors e1(h, r), . . . , e K(h, r), with a cost of c > 0 for abstention: Rrej gen(h, r) = ψ (e1(h, r), . . . , e K(h, r)) + c · P (r(x) = 1) . (14) This generalized L2R risk includes the balanced error from Eq. (6) as a special case when ψ(z) = 1 K P k zk. It also captures more general evaluation metrics deﬁned by a non-linear function ψ such as the worst-group error with ψ(z) = maxk∈[K] zk (Sagawa et al., 2020), and the G-mean metric when ψ(z) = 1 −(QK k=1(1 −zk))1/K (Menon et al., 2013; Narasimhan et al., 2015b). 5.1 BAYES-OPTIMAL CLASSIFIER AND REJECTOR As with prior work on optimizing general evaluation metrics (Narasimhan et al., 2015a; Yang et al., 2020), the generalized L2R risk is minimized by a classiﬁer-rejector mechanism that is stochastic. Theorem 3. For any non-linear ψ, there exists coefﬁcients u(1), v(1), . . . , u(K+1), v(K+1) ∈RK, and a distribution ν ∈∆K+1, such that, the generalized L2R risk in Eq. (14) is minimized by a stochastic mechanism that randomizes over K + 1 classiﬁer-rejector pairs: for any x, it predicts using (h(i), r(i)) with probability νi: h(i)(x) ∈arg maxy u(i) y · ηy(x), i ∈[K + 1] r(i)(x) = 1 ⇐⇒ maxy u(i) y · ηy(x) < P y′ v(i) y′ · ηy′(x) −c, i ∈[K + 1]. Stochastic classiﬁers are widely used in the design of learning algorithms for complex (non-linear) evaluation metrics (Narasimhan et al., 2015b; Cotter et al., 2019b; Narasimhan et al., 2019; Yang et al., 2020). In applications where the use of stochastic classiﬁers is infeasible, one may use techniques offered in Cotter et al. (2019a) to convert them into a similar performing deterministic classiﬁer. 7
Published as a conference paper at ICLR 2024 Method CIFAR-100 Image Net i Naturalist Balanced Worst Balanced Worst Balanced Worst Chow 0.509 ± 0.002 0.883 ± 0.007 0.409 ± 0.003 0.685 ± 0.007 0.131 ± 0.004 0.162 ± 0.004 CSS 0.483 ± 0.002 0.785 ± 0.003 0.526 ± 0.002 0.784 ± 0.003 0.200 ± 0.005 0.246 ± 0.006 Chow [BCE] 0.359 ± 0.017 0.570 ± 0.030 0.213 ± 0.001 0.274 ± 0.003 0.111 ± 0.003 0.114 ± 0.003 Chow [DRO] 0.325 ± 0.004 0.333 ± 0.003 0.197 ± 0.001 0.211 ± 0.004 0.110 ± 0.001 0.117 ± 0.002 Plug-in [Balanced] 0.292 ± 0.006 0.416 ± 0.015 0.205 ± 0.001 0.248 ± 0.001 0.108 ± 0.003 0.118 ± 0.004 Plug-in [Worst] 0.287 ± 0.008 0.321 ± 0.018 0.198 ± 0.001 0.213 ± 0.004 0.108 ± 0.001 0.113 ± 0.002 Table 2: Area Under the Risk-Coverage curve (AURC) with the balanced and worst-group error. Chow and CSS are L2R baselines. We evaluate two variants of Chow (BCE, DRO) with a modiﬁed base loss (§4.1), and plug-in methods (Balanced, Worst) that require no changes to the base model training (§4.2, §5). 5.2 MINIMIZING WORST-GROUP ERROR One can minimize the risk in Eq. (14) by exploiting the form of the function ψ. For example, if ψ is convex, one can reduce it to a sequence of minimization problems of the form: P k β(t) k · ek(h, r) + c · P (r(x) = 1) , for coefﬁcients β(1), β(2), . . . ∈RK + , each of which can be solved by adapting the plug-in method described in §4.2. Prior work has shown how this can be done in the standard classiﬁcation setup (without rejection) by applying gradient-based optimization to ψ (Narasimhan et al., 2019; 2022a; Yang et al., 2020). As a concrete example, we showcase how a similar reduction can be done for the worst-group L2R risk with ψ(z1, . . . , z K) = maxk∈[K] zk. The details are in Algorithm 2. The algorithm maintains group-speciﬁc weights β(t) ∈∆K, where ∆K is the probability simplex over K coordinates. At each iteration, it updates β(t) using exponentiated gradient-descent, up-weighting groups with high empirical error ˆek(h, r). It then invokes the plug-in approach in Algorithm 1 to minimize the resulting weighted error P k β(t) k · ˆek(h, r). Below, we provide convergence guarantees for the algorithm in terms of the optimality gap in the weighted error minimization step (line 4). Theorem 4. Let Rrej wst(h, r) denote the generalized L2R risk in Eq. (14) with ψ(z1, . . . , z K) = maxk zk. Let ϵgen t = |ek(h(t), r(t)) −ˆek(h(t), r(t))| denote the estimation error in iteration t of Algorithm 2. Let ϵcs t = P k β(t) k · ek(h(t), r(t)) −infh,r P k β(t) k · ek(h, r) denote the excess cost-sensitive risk for the classiﬁer-rejector pair (h(t), r(t)) returned in line 4. Then a stochastic mechanism that picks one among {(h(1), r(1)), . . . , (h(T ), r(T ))} with equal probability satisﬁes: max k∈[K] Et  ek(h(t), r(t))  + c · Et  P(r(t)(x) = 1)  ≤inf h,r Rrej wst(h, r) + ¯ϵcs + 2¯ϵgen + 2 q log(K) T , where t is drawn uniformly from {1, . . . , T}, ¯ϵcs = 1 T P t∈[T ] ϵcs t and ¯ϵgen = 1 T P t∈[T ] ϵgen t . To ensure a fair comparison against other deterministic baselines, when implementing Algorithm 2, we do not employ a stochastic solution, but instead use the ﬁnal classiﬁer-rejector pair (h(T ), r(T )). 6 EXPERIMENTAL RESULTS We present experiments on long-tailed image classiﬁcation tasks to showcase that proposed plug-in approaches for the balanced error (§4) and the worst-group error (§5) yield signiﬁcantly better trade-offs than Chow’s rule, despite using the same base model, and are competitive with variants of Chow’s rule which require re-training the base model with a modiﬁed loss. Datasets. We replicate the long-tail experimental setup from Menon et al. (2021a). We use long-tailed versions of CIFAR-100 (Krizhevsky, 2009), Image Net (Deng et al., 2009) and i Naturalist (Van Horn et al., 2018). For CIFAR-100, we downsample the examples per label following the Exp proﬁle of Cao et al. (2019), and for Image Net, we use the long-tail version provided by Liu et al. (2019). The train, test and validation splits have the same label distributions (se
